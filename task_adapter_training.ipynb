{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29d6932",
   "metadata": {
    "id": "30c9495f-baf7-4460-ab80-8860fb681e8e"
   },
   "source": [
    "## Training Task Adapters\n",
    "Using randomized search, we identify optimal hyperparameters to train task specfic adapters on GLUE tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36b615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14598,
     "status": "ok",
     "timestamp": 1627331551612,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "KdDh2RdkTRxi",
    "outputId": "0e461b18-4985-4f09-cb45-dfe2e2a47322"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bebb83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1627331553058,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "BQjODWExTcpq",
    "outputId": "e92c101a-3cb2-4243-c0dc-3c398b6333ba"
   },
   "outputs": [],
   "source": [
    "# cd drive/MyDrive/cs7643-deep-learning-summer-2021/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e8cb96-3d93-4d2f-a448-d908768a6af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16466,
     "status": "ok",
     "timestamp": 1627331572588,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "9889c970-bdb0-464f-953d-78c2224b76fe",
    "outputId": "c6ed4418-74d9-425b-cd05-e53d99f74100"
   },
   "outputs": [],
   "source": [
    "# !pip install -Uqq adapter-transformers datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "from typing import Dict, List\n",
    "from task_utils import TaskModelArguments, TaskDataTrainingArguments\n",
    "from task import train_task_adapter\n",
    "from transformers import (\n",
    "    MultiLingAdapterArguments,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9d3eb",
   "metadata": {
    "id": "33aa90b1-82cb-41e8-b38d-e6fceb42f486"
   },
   "source": [
    "### Utility Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e718ccd-ba80-4b24-8476-7b5652d1b687",
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1627331575013,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "oEFf_pUPYdWU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def getParams(dictionary, limit):\n",
    "    paramsList = [dict(zip(dictionary, v)) for v in itertools.product(*dictionary.values())]\n",
    "    random.shuffle(paramsList)\n",
    "\n",
    "    if limit is not False:\n",
    "        paramsList = paramsList[0:min(limit, len(paramsList))]\n",
    "\n",
    "    return paramsList\n",
    "\n",
    "def initParse(dictionary: Dict, output_prefix = \"\"):\n",
    "    model = TaskModelArguments(\n",
    "        model_name_or_path=dictionary.get('model_name_or_path')\n",
    "    )\n",
    "\n",
    "    data = TaskDataTrainingArguments(\n",
    "        task_name=dictionary.get('task_name'),\n",
    "        max_seq_length=dictionary.get('max_seq_length'),\n",
    "        pad_to_max_length=dictionary.get('pad_to_max_length')\n",
    "    )\n",
    "\n",
    "    training = TrainingArguments(\n",
    "        adam_beta1=dictionary.get('adam_beta1'),\n",
    "        adam_beta2=dictionary.get('adam_beta2'),\n",
    "        adam_epsilon=dictionary.get('adam_epsilon'),\n",
    "        learning_rate=dictionary.get('learning_rate'),\n",
    "        fp16=dictionary.get('fp16'),\n",
    "        warmup_ratio=dictionary.get('warmup_ratio'),\n",
    "        warmup_steps=dictionary.get('warmup_steps'),\n",
    "        weight_decay=dictionary.get('weight_decay'),\n",
    "        do_train=dictionary.get('do_train'),\n",
    "        do_eval=dictionary.get('do_train'),\n",
    "        per_device_train_batch_size=dictionary.get('per_device_train_batch_size'),\n",
    "        num_train_epochs=dictionary.get('num_train_epochs'), # CHANGE ME\n",
    "        overwrite_output_dir=dictionary.get('overwrite_output_dir'),\n",
    "        output_dir=f\"./adapter/task/{output_prefix}{dictionary.get('task_name')}\",\n",
    "    )\n",
    "\n",
    "    adapter = MultiLingAdapterArguments(\n",
    "        train_adapter=True,\n",
    "        adapter_config=\"pfeiffer\",\n",
    "    )\n",
    "\n",
    "    return model, data, training, adapter\n",
    "\n",
    "def train(params: Dict, output_prefix = \"\") -> List:\n",
    "    model, data, training, adapter = initParse(params, output_prefix)\n",
    "    \n",
    "    train_stats, eval_stats = train_task_adapter(\n",
    "        model_args=model, \n",
    "        adapter_args=adapter, \n",
    "        training_args=training, \n",
    "        data_args=data\n",
    "    )\n",
    "    \n",
    "    row = []\n",
    "    row.extend(list(params.values()))\n",
    "    row.extend(list(train_stats.values()))\n",
    "    row.extend(list(eval_stats.values()))\n",
    "    \n",
    "    header = []\n",
    "    header.extend(list(params.keys()))\n",
    "    header.extend(list(train_stats.keys()))\n",
    "    header.extend(list(eval_stats.keys()))\n",
    "\n",
    "    output_df = pd.DataFrame([row], columns=header)\n",
    "    \n",
    "    del model\n",
    "    del data\n",
    "    del training\n",
    "    del adapter\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22451996-72ae-4fe8-8a65-e6ecc89e25e4",
   "metadata": {},
   "source": [
    "## Random Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb130ac9",
   "metadata": {
    "id": "FZR2TOk1hZyT",
    "tags": []
   },
   "source": [
    "**Define Dictionary of Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc3030f-43e4-4063-a922-a4376ed66743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glue_tasks = [\n",
    "    \"cola\",\n",
    "    #\"mnli\",\n",
    "    #\"mrpc\",\n",
    "    #\"qnli\",\n",
    "    #\"qqp\",\n",
    "    #\"rte\",\n",
    "    #sst2\",\n",
    "    #\"stsb\",\n",
    "    #\"wnli\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816a7b2f",
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1627331583420,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "F1FN14CdVqFP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "task = 'cola'\n",
    "paramDictionary = {\n",
    "    'task_name':[task],\n",
    "    'model_name_or_path':['roberta-base'],\n",
    "    'max_seq_length':[64, 128, 256],\n",
    "    'pad_to_max_length':[True],\n",
    "    'per_device_train_batch_size':[16, 32, 64],\n",
    "    'adam_beta1':[.9],\n",
    "    'adam_beta2':[.999],\n",
    "    'adam_epsilon':[1e-8,1e-7,1e-6],\n",
    "    'fp16':[True],\n",
    "    'learning_rate':[1e-5,5e-5,1e-4,5e-4,1e-3],\n",
    "    'warmup_ratio':[0.0],\n",
    "    'warmup_steps':[0],\n",
    "    'weight_decay':[0.0],\n",
    "    'do_train':[True],\n",
    "    'do_eval':[True],\n",
    "    'num_train_epochs':[10],\n",
    "    'overwrite_output_dir':[True],\n",
    "    'adapter_config':['pfeiffer']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221eee26",
   "metadata": {
    "id": "CHmh6eeThiTm",
    "tags": []
   },
   "source": [
    "**Begin Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2372beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "beab7a6853464a2fb9da17c3a0c53f80",
      "06e9193555754f3c820fb47c9322925d",
      "d9c12bc752e745149f3d3e0edf93fe10",
      "8d12e917320e4de7ad3bd165a4f991bf",
      "7f5b43aafb754ccd82c11b0f7fa6b210",
      "2d64c717a1354cefab8b3c9e6334577e",
      "99a900f85059444581e93cb6ec686c4d",
      "3c3c6c0c806e417db227ad8afe80335e",
      "ccac7134e00a4f6581d80ca2943a0fcf",
      "3ee4cb3cecf0451f925dc34ed709d2a0",
      "c8bbed77890e4deb9d9263c95310a42a",
      "0a67106bdf5740baa70ccdced2ddc14e",
      "7ec6b5f9910f4a6da32473732eb96508",
      "5a12798be0d44c5fbbcfff260f366b68",
      "0e0ea8ae0cca4ab0914bb608a721aa18",
      "c00d73694fdb483d9d1f2a0e4334e890",
      "7a4a19a374ca47e5ace3a71fef6f4179",
      "bdea7284db604e26af00969e632fe1d3",
      "bcd6ce6c84574455ba3dd425e2267241",
      "310079aaacc443a98064000f342261a8",
      "a8915cd5793a408092c6d0e7357c6ba1",
      "83aa40a78b5e4f5593190fb8751f1f21"
     ]
    },
    "executionInfo": {
     "elapsed": 885367,
     "status": "ok",
     "timestamp": 1627335786995,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "uEcrWpiJbyed",
    "outputId": "f517ea12-a23e-4b6e-bd83-62614f8acf1c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "limit = 15 #Numerical or False for no limit\n",
    "\n",
    "for data_set in glue_tasks:\n",
    "    paramDictionary[\"task_name\"] = [data_set]\n",
    "    paramsList = getParams(paramDictionary, limit)\n",
    "\n",
    "    results = None\n",
    "    for p in paramsList:\n",
    "        trial_data = train(p)\n",
    "        \n",
    "        if results is not None:\n",
    "            results = results.append(trial_data)\n",
    "        else:\n",
    "            results = trial_data #first pass through the loop\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    results.to_csv(f\"./adapter/task/{data_set}_hp_search.{time():.0f}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88958964-0df8-4786-b69b-61d702e2d7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4d8a3-2f84-4dde-ab1c-432319390c87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_csv(f\"./adapter/task/{data_set}_hp_search.{time():.0f}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a46d41-3028-4371-b7ba-e2af84c03004",
   "metadata": {},
   "source": [
    "## Final Training\n",
    "Training each adapter again with the optimal settings discovered through the random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609c37e0-d2c3-451d-b9eb-2d1a51285e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "def final_training(\n",
    "    task, \n",
    "    learning_rate, \n",
    "    max_seq_length, \n",
    "    per_device_train_batch_size, \n",
    "    adam_epsilon,\n",
    "    num_train_epochs\n",
    "    ):\n",
    "    \n",
    "    home = str(Path.home())\n",
    "    model_dir = f\"{home}/git/roberta-base\"\n",
    "    \n",
    "    copy_adapter_config(task_name=task, model_dir=model_dir)\n",
    "    \n",
    "    final_params = {\n",
    "        'task_name':[task],\n",
    "        'model_name_or_path':[model_dir],\n",
    "        'max_seq_length':[max_seq_length],\n",
    "        'pad_to_max_length':[True],\n",
    "        'per_device_train_batch_size':[per_device_train_batch_size],\n",
    "        'adam_beta1':[.9],\n",
    "        'adam_beta2':[.999],\n",
    "        'adam_epsilon':[adam_epsilon],\n",
    "        'fp16':[True],\n",
    "        'learning_rate':[learning_rate],\n",
    "        'warmup_ratio':[0.0],\n",
    "        'warmup_steps':[0],\n",
    "        'weight_decay':[0.0],\n",
    "        'do_train':[True],\n",
    "        'do_eval':[True],\n",
    "        'num_train_epochs':[num_train_epochs],\n",
    "        'overwrite_output_dir':[True],\n",
    "        'adapter_config':[f\"pfeiffer\"],\n",
    "    }\n",
    "    \n",
    "    prefix = \"final_\"\n",
    "    p = getParams(final_params, 1)\n",
    "    result = train(params=p[0], output_prefix=prefix)\n",
    "    result.to_csv(f\"./adapter/task/{prefix}{task}_hp_search.{time():.0f}.csv\")\n",
    "    \n",
    "def copy_adapter_config(task_name:str, model_dir:str):\n",
    "    \"\"\"Copy the adapter config into the downloaded local model location\"\"\"\n",
    "    \n",
    "    config_location = f\"./adapter/task/{task_name}/{task_name}\"\n",
    "    \n",
    "    copyfile(src=f\"{config_location}/adapter_config.json\", dst=f\"{model_dir}/.git/adapter_config.json\")\n",
    "    copyfile(src=f\"{config_location}/pytorch_adapter.bin\", dst=f\"{model_dir}/.git/pytorch_adapter.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0af122c-52d9-47c4-9c12-9ad2846d489a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "glue_tasks = [\"sst2\", \"cola\", \"wnli\", \"rte\", \"qnli\"]\n",
    "final_params = {\n",
    "    \"sst2\" : {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"max_seq_length\": 64,\n",
    "        \"per_device_train_batch_size\": 32,\n",
    "        \"adam_epsilon\": 1e-7,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "    \"cola\" : {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_seq_length\": 256,\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"adam_epsilon\": 1e-7,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "    \"wnli\" : {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"max_seq_length\": 256,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "    \"rte\" : {\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"max_seq_length\": 256,\n",
    "        \"per_device_train_batch_size\": 16,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "    \"qnli\" : {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"max_seq_length\": 128,\n",
    "        \"per_device_train_batch_size\": 64,\n",
    "        \"adam_epsilon\": 1e-7,\n",
    "        \"num_train_epochs\": 10,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c522664-37b4-4c4b-94a4-e2dcc4e66739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### START TASK: sst2 #####\n",
      "{'learning_rate': 0.0005, 'max_seq_length': 64, 'per_device_train_batch_size': 32, 'adam_epsilon': 1e-07, 'num_train_epochs': 10}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 15:44:44 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True\n",
      "07/31/2021 15:44:44 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-07,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/final_sst2/runs/Jul31_15-44-41_ip-172-16-1-120,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/final_sst2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=final_sst2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/final_sst2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/31/2021 15:44:44 - WARNING - datasets.builder -   Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"sst2\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /home/ubuntu/git/roberta-base/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/tokenizer_config.json. We won't load it.\n",
      "loading file /home/ubuntu/git/roberta-base/vocab.json\n",
      "loading file /home/ubuntu/git/roberta-base/merges.txt\n",
      "loading file /home/ubuntu/git/roberta-base/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /home/ubuntu/git/roberta-base/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/ubuntu/git/roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/ubuntu/git/roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'sst2' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'negative': 0, 'positive': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'sst2'.\n",
      "07/31/2021 15:44:45 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1aeae2e9685f7340.arrow\n",
      "07/31/2021 15:44:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a1a51d2078d4249c.arrow\n",
      "07/31/2021 15:44:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ba6b76ea4198fc07.arrow\n",
      "07/31/2021 15:44:46 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 102, 372, 1569, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'a great movie '}.\n",
      "07/31/2021 15:44:46 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 1342, 2399, 8173, 2156, 114, 5568, 28631, 2156, 814, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'entertaining , if somewhat standardized , action '}.\n",
      "07/31/2021 15:44:46 - INFO - task -   Sample 36048 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 36048, 'input_ids': [0, 12963, 77, 89, 32, 29620, 29, 2156, 5, 8597, 2045, 12757, 2156, 1437, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'even when there are lulls , the emotions seem authentic , '}.\n",
      "Using amp fp16 backend\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "Loading model from /home/ubuntu/git/roberta-base).\n",
      "Loading module configuration from /home/ubuntu/git/roberta-base/.git/adapter_config.json\n",
      "Overwriting existing adapter 'sst2'.\n",
      "Loading module weights from /home/ubuntu/git/roberta-base/.git/pytorch_adapter.bin\n",
      "No matching prediction head found in '/home/ubuntu/git/roberta-base/.git'\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: idx, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5270\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5270' max='5270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5270/5270 26:28, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.144400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.123700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-500\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-500/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-500/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-500/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-500/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1000/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-1000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-1500/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-1500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2000/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-2000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-2500/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-2500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-3000\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3000/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-3000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-3500\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-3500/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-3500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-4000\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4000/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-4000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-4500\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-4500/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-4500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_sst2/checkpoint-5000\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/checkpoint-5000/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/checkpoint-5000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/final_sst2\n",
      "Configuration saved in ./adapter/task/final_sst2/sst2/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/sst2/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/sst2/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_sst2/sst2/head_config.json\n",
      "Module weights saved in ./adapter/task/final_sst2/sst2/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_sst2/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_sst2/special_tokens_map.json\n",
      "07/31/2021 16:11:30 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: idx, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 872\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:11:33 - INFO - task -   ***** Eval results sst2 *****\n",
      "07/31/2021 16:11:33 - INFO - task -     eval_loss = 0.215029776096344\n",
      "07/31/2021 16:11:33 - INFO - task -     eval_accuracy = 0.9495412844036697\n",
      "07/31/2021 16:11:33 - INFO - task -     eval_runtime = 3.4394\n",
      "07/31/2021 16:11:33 - INFO - task -     eval_samples_per_second = 253.53\n",
      "07/31/2021 16:11:33 - INFO - task -     eval_steps_per_second = 8.141\n",
      "07/31/2021 16:11:33 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/31/2021 16:11:33 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True\n",
      "07/31/2021 16:11:33 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-07,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/final_cola/runs/Jul31_16-11-33_ip-172-16-1-120,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/final_cola,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=final_cola,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/final_cola,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### START TASK: cola #####\n",
      "{'learning_rate': 0.001, 'max_seq_length': 256, 'per_device_train_batch_size': 64, 'adam_epsilon': 1e-07, 'num_train_epochs': 10}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:11:33 - WARNING - datasets.builder -   Reusing dataset glue (/home/ubuntu/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"cola\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /home/ubuntu/git/roberta-base/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/tokenizer_config.json. We won't load it.\n",
      "loading file /home/ubuntu/git/roberta-base/vocab.json\n",
      "loading file /home/ubuntu/git/roberta-base/merges.txt\n",
      "loading file /home/ubuntu/git/roberta-base/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /home/ubuntu/git/roberta-base/pytorch_model.bin\n",
      "Some weights of the model checkpoint at /home/ubuntu/git/roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/ubuntu/git/roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'cola' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'unacceptable': 0, 'acceptable': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'cola'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aae1c612d94e0d96b05c2e00b0f09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e06a1a6f88942568b9676433085fcc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f7703ea61646318d0b13f5556b8641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:11:35 - INFO - task -   Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1824, 'input_ids': [0, 100, 5055, 14, 127, 1150, 6, 37, 21, 3229, 25, 41, 37323, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence': 'I acknowledged that my father, he was tight as an owl.'}.\n",
      "07/31/2021 16:11:35 - INFO - task -   Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 409, 'input_ids': [0, 2709, 123, 7, 109, 14, 74, 28, 10, 5021, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'For him to do that would be a mistake.'}.\n",
      "07/31/2021 16:11:35 - INFO - task -   Sample 4506 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 4506, 'input_ids': [0, 24877, 11944, 10, 2214, 6, 53, 2094, 393, 222, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence': 'Mary sang a song, but Lee never did.'}.\n",
      "Using amp fp16 backend\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "Loading model from /home/ubuntu/git/roberta-base).\n",
      "Loading module configuration from /home/ubuntu/git/roberta-base/.git/adapter_config.json\n",
      "Overwriting existing adapter 'cola'.\n",
      "Loading module weights from /home/ubuntu/git/roberta-base/.git/pytorch_adapter.bin\n",
      "No matching prediction head found in '/home/ubuntu/git/roberta-base/.git'\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: idx, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 8551\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 340\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 06:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/final_cola\n",
      "Configuration saved in ./adapter/task/final_cola/cola/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_cola/cola/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_cola/cola/head_config.json\n",
      "Module weights saved in ./adapter/task/final_cola/cola/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_cola/cola/head_config.json\n",
      "Module weights saved in ./adapter/task/final_cola/cola/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_cola/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_cola/special_tokens_map.json\n",
      "07/31/2021 16:18:03 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: idx, sentence.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1043\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:18:10 - INFO - task -   ***** Eval results cola *****\n",
      "07/31/2021 16:18:10 - INFO - task -     eval_loss = 0.5718619227409363\n",
      "07/31/2021 16:18:10 - INFO - task -     eval_matthews_correlation = 0.5879831868448624\n",
      "07/31/2021 16:18:10 - INFO - task -     eval_runtime = 7.3667\n",
      "07/31/2021 16:18:10 - INFO - task -     eval_samples_per_second = 141.583\n",
      "07/31/2021 16:18:10 - INFO - task -     eval_steps_per_second = 4.48\n",
      "07/31/2021 16:18:10 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/31/2021 16:18:10 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True\n",
      "07/31/2021 16:18:10 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/final_wnli/runs/Jul31_16-18-10_ip-172-16-1-120,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/final_wnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=final_wnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/final_wnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### START TASK: wnli #####\n",
      "{'learning_rate': 1e-05, 'max_seq_length': 256, 'per_device_train_batch_size': 16, 'adam_epsilon': 1e-06, 'num_train_epochs': 10}\n",
      "\n",
      "\n",
      "Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e53ca9333114684bf219d928f64703e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"wnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /home/ubuntu/git/roberta-base/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/tokenizer_config.json. We won't load it.\n",
      "loading file /home/ubuntu/git/roberta-base/vocab.json\n",
      "loading file /home/ubuntu/git/roberta-base/merges.txt\n",
      "loading file /home/ubuntu/git/roberta-base/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /home/ubuntu/git/roberta-base/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/git/roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/ubuntu/git/roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'wnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'not_entailment': 0, 'entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'wnli'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ae76a5d782419eb4415deb85302cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfd4ee64a1b47598fc37b40bf195ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03194ec361d46d0a77572b1d5640311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:18:13 - INFO - task -   Sample 114 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 114, 'input_ids': [0, 1121, 550, 6, 5911, 90, 29465, 2348, 2998, 997, 15, 19810, 7046, 330, 4, 1773, 19810, 7046, 330, 18, 3835, 21, 203, 357, 8895, 8, 2724, 498, 2514, 6, 51, 58, 5125, 624, 688, 4, 2, 2, 975, 677, 7046, 330, 21, 5125, 624, 688, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.'}.\n",
      "07/31/2021 16:18:13 - INFO - task -   Sample 25 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 25, 'input_ids': [0, 133, 8071, 630, 75, 2564, 88, 5, 6219, 28441, 142, 24, 16, 350, 739, 4, 2, 2, 133, 28441, 16, 350, 739, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.'}.\n",
      "07/31/2021 16:18:13 - INFO - task -   Sample 281 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 281, 'input_ids': [0, 12083, 1381, 7, 486, 1655, 15, 5, 1028, 6, 53, 37, 938, 75, 1800, 4, 2, 2, 25395, 938, 75, 1800, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\"}.\n",
      "Using amp fp16 backend\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "Loading model from /home/ubuntu/git/roberta-base).\n",
      "Loading module configuration from /home/ubuntu/git/roberta-base/.git/adapter_config.json\n",
      "Overwriting existing adapter 'wnli'.\n",
      "Loading module weights from /home/ubuntu/git/roberta-base/.git/pytorch_adapter.bin\n",
      "No matching prediction head found in '/home/ubuntu/git/roberta-base/.git'\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence1, sentence2, idx.\n",
      "***** Running training *****\n",
      "  Num examples = 635\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/final_wnli\n",
      "Configuration saved in ./adapter/task/final_wnli/wnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_wnli/wnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_wnli/wnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_wnli/wnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_wnli/wnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_wnli/wnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_wnli/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_wnli/special_tokens_map.json\n",
      "07/31/2021 16:18:57 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence1, sentence2, idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 71\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:18:58 - INFO - task -   ***** Eval results wnli *****\n",
      "07/31/2021 16:18:58 - INFO - task -     eval_loss = 0.6883140206336975\n",
      "07/31/2021 16:18:58 - INFO - task -     eval_accuracy = 0.5633802816901409\n",
      "07/31/2021 16:18:58 - INFO - task -     eval_runtime = 0.5812\n",
      "07/31/2021 16:18:58 - INFO - task -     eval_samples_per_second = 122.155\n",
      "07/31/2021 16:18:58 - INFO - task -     eval_steps_per_second = 5.161\n",
      "07/31/2021 16:18:58 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/31/2021 16:18:58 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True\n",
      "07/31/2021 16:18:58 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/final_rte/runs/Jul31_16-18-58_ip-172-16-1-120,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/final_rte,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=final_rte,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/final_rte,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### START TASK: rte #####\n",
      "{'learning_rate': 0.0005, 'max_seq_length': 256, 'per_device_train_batch_size': 16, 'adam_epsilon': 1e-08, 'num_train_epochs': 10}\n",
      "\n",
      "\n",
      "Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51726a72e02547508513051cee51b6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/697k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /home/ubuntu/git/roberta-base/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/tokenizer_config.json. We won't load it.\n",
      "loading file /home/ubuntu/git/roberta-base/vocab.json\n",
      "loading file /home/ubuntu/git/roberta-base/merges.txt\n",
      "loading file /home/ubuntu/git/roberta-base/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /home/ubuntu/git/roberta-base/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/git/roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/ubuntu/git/roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'rte' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'rte'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0379d441d9e44fe9a51b5e73199947aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f61e83efcc64bf2861764c5cb009e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35257f94254544e5bb39a6e20d02a337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:19:02 - INFO - task -   Sample 456 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 456, 'input_ids': [0, 250, 3034, 467, 2988, 1367, 159, 458, 1446, 23, 5, 5308, 3412, 3080, 13, 144, 9, 2350, 6, 5, 2373, 10044, 7, 1248, 13, 1817, 18, 1154, 741, 21117, 4, 2, 2, 133, 5308, 3412, 3080, 21, 1367, 159, 30, 3034, 467, 2988, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'sentence1': \"A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.\", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.'}.\n",
      "07/31/2021 16:19:02 - INFO - task -   Sample 102 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 102, 'input_ids': [0, 487, 23065, 7194, 39, 563, 7, 671, 62, 7, 8963, 6, 151, 82, 7, 5, 343, 6, 624, 10, 186, 8, 10, 457, 6, 1135, 1379, 59, 5, 765, 1787, 9, 4835, 514, 8, 4008, 30002, 5005, 21648, 4, 2, 2, 33383, 9, 82, 32, 421, 7, 671, 7, 188, 4942, 42, 186, 6, 25, 911, 9, 5, 343, 32, 1357, 62, 7, 1196, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.'}.\n",
      "07/31/2021 16:19:02 - INFO - task -   Sample 1126 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 1126, 'input_ids': [0, 1121, 15077, 6, 37, 4855, 62, 5, 266, 15670, 5, 27020, 31, 5, 537, 9, 5, 41373, 139, 333, 4, 96, 11724, 6, 71, 4323, 1527, 7414, 18, 744, 6, 234, 10460, 21, 2736, 25, 537, 2971, 4, 2, 2, 487, 10460, 2800, 5, 41373, 139, 333, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'sentence1': \"In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.\", 'sentence2': 'Natta supported the Manifesto group.'}.\n",
      "Using amp fp16 backend\n",
      "Loading model from /home/ubuntu/git/roberta-base).\n",
      "Loading module configuration from /home/ubuntu/git/roberta-base/.git/adapter_config.json\n",
      "Overwriting existing adapter 'rte'.\n",
      "Loading module weights from /home/ubuntu/git/roberta-base/.git/pytorch_adapter.bin\n",
      "No matching prediction head found in '/home/ubuntu/git/roberta-base/.git'\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence1, sentence2, idx.\n",
      "***** Running training *****\n",
      "  Num examples = 2490\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 390\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 02:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/final_rte\n",
      "Configuration saved in ./adapter/task/final_rte/rte/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_rte/rte/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_rte/rte/head_config.json\n",
      "Module weights saved in ./adapter/task/final_rte/rte/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_rte/rte/head_config.json\n",
      "Module weights saved in ./adapter/task/final_rte/rte/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_rte/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_rte/special_tokens_map.json\n",
      "07/31/2021 16:21:53 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence1, sentence2, idx.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:21:55 - INFO - task -   ***** Eval results rte *****\n",
      "07/31/2021 16:21:55 - INFO - task -     eval_loss = 0.7079261541366577\n",
      "07/31/2021 16:21:55 - INFO - task -     eval_accuracy = 0.7581227436823105\n",
      "07/31/2021 16:21:55 - INFO - task -     eval_runtime = 2.0077\n",
      "07/31/2021 16:21:55 - INFO - task -     eval_samples_per_second = 137.972\n",
      "07/31/2021 16:21:55 - INFO - task -     eval_steps_per_second = 4.483\n",
      "07/31/2021 16:21:55 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/31/2021 16:21:55 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 4distributed training: False, 16-bits training: True\n",
      "07/31/2021 16:21:55 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-07,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/final_qnli/runs/Jul31_16-21-55_ip-172-16-1-120,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/final_qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=final_qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/final_qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### START TASK: qnli #####\n",
      "{'learning_rate': 0.001, 'max_seq_length': 128, 'per_device_train_batch_size': 64, 'adam_epsilon': 1e-07, 'num_train_epochs': 10}\n",
      "\n",
      "\n",
      "Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /home/ubuntu/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3227b3e51e4d5a990c387508972f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"qnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file /home/ubuntu/git/roberta-base/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /home/ubuntu/git/roberta-base/added_tokens.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/special_tokens_map.json. We won't load it.\n",
      "Didn't find file /home/ubuntu/git/roberta-base/tokenizer_config.json. We won't load it.\n",
      "loading file /home/ubuntu/git/roberta-base/vocab.json\n",
      "loading file /home/ubuntu/git/roberta-base/merges.txt\n",
      "loading file /home/ubuntu/git/roberta-base/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading weights file /home/ubuntu/git/roberta-base/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/git/roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /home/ubuntu/git/roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'qnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'qnli'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dca1eb728964c978e5c5a52a5b161a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/105 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eedd82c80140e6860d49fd3d5734f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3423cc5c7f11441fb9f6b42ce4fec62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/31/2021 16:22:07 - INFO - task -   Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 996, 6, 151, 42669, 43, 58, 35578, 13, 5, 221, 9788, 361, 212, 2938, 826, 18, 130, 12, 4862, 1657, 196, 9689, 21163, 13767, 8893, 23, 5, 9846, 9, 732, 366, 179, 23895, 13878, 6, 53, 51, 2312, 7, 5111, 223, 1754, 3177, 8, 1577, 8848, 323, 668, 578, 41324, 19, 103, 379, 6, 151, 6981, 12675, 4, 2, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
      "07/31/2021 16:22:07 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
      "07/31/2021 16:22:07 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n",
      "Using amp fp16 backend\n",
      "Loading model from /home/ubuntu/git/roberta-base).\n",
      "Loading module configuration from /home/ubuntu/git/roberta-base/.git/adapter_config.json\n",
      "Overwriting existing adapter 'qnli'.\n",
      "Loading module weights from /home/ubuntu/git/roberta-base/.git/pytorch_adapter.bin\n",
      "No matching prediction head found in '/home/ubuntu/git/roberta-base/.git'\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: question, idx, sentence.\n",
      "***** Running training *****\n",
      "  Num examples = 104743\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2907' max='4100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2907/4100 29:53 < 12:16, 1.62 it/s, Epoch 7.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./adapter/task/final_qnli/checkpoint-500\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_qnli/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_qnli/checkpoint-500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/final_qnli/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_qnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_qnli/checkpoint-1000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_qnli/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_qnli/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_qnli/checkpoint-1500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_qnli/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_qnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_qnli/checkpoint-2000/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./adapter/task/final_qnli/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/final_qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/final_qnli/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/final_qnli/checkpoint-2500/special_tokens_map.json\n",
      "/home/ubuntu/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-h1yaMy-h/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "for task in glue_tasks:\n",
    "    \n",
    "    print(f\"\\n\\n##### START TASK: {task} #####\\n{final_params.get(task)}\\n\\n\")\n",
    "    \n",
    "    final_training(task=task,\n",
    "                   **final_params.get(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becca06-6867-4070-9fa4-83e17b81d75e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task_adapter_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06e9193555754f3c820fb47c9322925d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a67106bdf5740baa70ccdced2ddc14e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a12798be0d44c5fbbcfff260f366b68",
       "IPY_MODEL_0e0ea8ae0cca4ab0914bb608a721aa18",
       "IPY_MODEL_c00d73694fdb483d9d1f2a0e4334e890"
      ],
      "layout": "IPY_MODEL_7ec6b5f9910f4a6da32473732eb96508"
     }
    },
    "0e0ea8ae0cca4ab0914bb608a721aa18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310079aaacc443a98064000f342261a8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bcd6ce6c84574455ba3dd425e2267241",
      "value": 2
     }
    },
    "2d64c717a1354cefab8b3c9e6334577e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "310079aaacc443a98064000f342261a8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3c6c0c806e417db227ad8afe80335e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ee4cb3cecf0451f925dc34ed709d2a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a12798be0d44c5fbbcfff260f366b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdea7284db604e26af00969e632fe1d3",
      "placeholder": "​",
      "style": "IPY_MODEL_7a4a19a374ca47e5ace3a71fef6f4179",
      "value": "100%"
     }
    },
    "7a4a19a374ca47e5ace3a71fef6f4179": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec6b5f9910f4a6da32473732eb96508": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f5b43aafb754ccd82c11b0f7fa6b210": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8bbed77890e4deb9d9263c95310a42a",
      "placeholder": "​",
      "style": "IPY_MODEL_3ee4cb3cecf0451f925dc34ed709d2a0",
      "value": " 2/2 [00:00&lt;00:00,  9.47ba/s]"
     }
    },
    "83aa40a78b5e4f5593190fb8751f1f21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d12e917320e4de7ad3bd165a4f991bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccac7134e00a4f6581d80ca2943a0fcf",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c3c6c0c806e417db227ad8afe80335e",
      "value": 2
     }
    },
    "99a900f85059444581e93cb6ec686c4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8915cd5793a408092c6d0e7357c6ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcd6ce6c84574455ba3dd425e2267241": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdea7284db604e26af00969e632fe1d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beab7a6853464a2fb9da17c3a0c53f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9c12bc752e745149f3d3e0edf93fe10",
       "IPY_MODEL_8d12e917320e4de7ad3bd165a4f991bf",
       "IPY_MODEL_7f5b43aafb754ccd82c11b0f7fa6b210"
      ],
      "layout": "IPY_MODEL_06e9193555754f3c820fb47c9322925d"
     }
    },
    "c00d73694fdb483d9d1f2a0e4334e890": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83aa40a78b5e4f5593190fb8751f1f21",
      "placeholder": "​",
      "style": "IPY_MODEL_a8915cd5793a408092c6d0e7357c6ba1",
      "value": " 2/2 [00:00&lt;00:00,  6.16ba/s]"
     }
    },
    "c8bbed77890e4deb9d9263c95310a42a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccac7134e00a4f6581d80ca2943a0fcf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c12bc752e745149f3d3e0edf93fe10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a900f85059444581e93cb6ec686c4d",
      "placeholder": "​",
      "style": "IPY_MODEL_2d64c717a1354cefab8b3c9e6334577e",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
