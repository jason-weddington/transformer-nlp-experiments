{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29d6932",
   "metadata": {
    "id": "30c9495f-baf7-4460-ab80-8860fb681e8e"
   },
   "source": [
    "## Training Task Adapters\n",
    "Using randomized search, we identify optimal hyperparameters to train task specfic adapters on GLUE tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a36b615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14598,
     "status": "ok",
     "timestamp": 1627331551612,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "KdDh2RdkTRxi",
    "outputId": "0e461b18-4985-4f09-cb45-dfe2e2a47322"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72bebb83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1627331553058,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "BQjODWExTcpq",
    "outputId": "e92c101a-3cb2-4243-c0dc-3c398b6333ba"
   },
   "outputs": [],
   "source": [
    "# cd drive/MyDrive/cs7643-deep-learning-summer-2021/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e8cb96-3d93-4d2f-a448-d908768a6af1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16466,
     "status": "ok",
     "timestamp": 1627331572588,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "9889c970-bdb0-464f-953d-78c2224b76fe",
    "outputId": "c6ed4418-74d9-425b-cd05-e53d99f74100"
   },
   "outputs": [],
   "source": [
    "# !pip install -Uqq adapter-transformers datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from time import time\n",
    "from typing import Dict, List\n",
    "from task_utils import TaskModelArguments, TaskDataTrainingArguments\n",
    "from task import train_task_adapter\n",
    "from transformers import (\n",
    "    MultiLingAdapterArguments,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9d3eb",
   "metadata": {
    "id": "33aa90b1-82cb-41e8-b38d-e6fceb42f486"
   },
   "source": [
    "### CoLA Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e718ccd-ba80-4b24-8476-7b5652d1b687",
   "metadata": {
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1627331575013,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "oEFf_pUPYdWU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def getParams(dictionary, limit):\n",
    "    paramsList = [dict(zip(dictionary, v)) for v in itertools.product(*dictionary.values())]\n",
    "    random.shuffle(paramsList)\n",
    "\n",
    "    if limit is not False:\n",
    "        paramsList = paramsList[0:min(limit, len(paramsList))]\n",
    "\n",
    "    return paramsList\n",
    "\n",
    "def initParse(dictionary: Dict):\n",
    "    model = TaskModelArguments(\n",
    "        model_name_or_path=dictionary.get('model_name_or_path')\n",
    "    )\n",
    "\n",
    "    data = TaskDataTrainingArguments(\n",
    "        task_name=dictionary.get('task_name'),\n",
    "        max_seq_length=dictionary.get('max_seq_length'),\n",
    "        pad_to_max_length=dictionary.get('pad_to_max_length')\n",
    "    )\n",
    "\n",
    "    training = TrainingArguments(\n",
    "        adam_beta1=dictionary.get('adam_beta1'),\n",
    "        adam_beta2=dictionary.get('adam_beta2'),\n",
    "        adam_epsilon=dictionary.get('adam_epsilon'),\n",
    "        learning_rate=dictionary.get('learning_rate'),\n",
    "        fp16=dictionary.get('fp16'),\n",
    "        warmup_ratio=dictionary.get('warmup_ratio'),\n",
    "        warmup_steps=dictionary.get('warmup_steps'),\n",
    "        weight_decay=dictionary.get('weight_decay'),\n",
    "        do_train=dictionary.get('do_train'),\n",
    "        do_eval=dictionary.get('do_train'),\n",
    "        per_device_train_batch_size=dictionary.get('per_device_train_batch_size'),\n",
    "        num_train_epochs=dictionary.get('num_train_epochs'), # CHANGE ME\n",
    "        overwrite_output_dir=dictionary.get('overwrite_output_dir'),\n",
    "        output_dir=f\"./adapter/task/{dictionary.get('task_name')}\",\n",
    "    )\n",
    "\n",
    "    adapter = MultiLingAdapterArguments(\n",
    "        train_adapter=True,\n",
    "        adapter_config=\"pfeiffer\",\n",
    "    )\n",
    "\n",
    "    return model, data, training, adapter\n",
    "\n",
    "def train(params: Dict) -> List:\n",
    "    model, data, training, adapter = initParse(params)\n",
    "    \n",
    "    train_stats, eval_stats = train_task_adapter(\n",
    "        model_args=model, \n",
    "        adapter_args=adapter, \n",
    "        training_args=training, \n",
    "        data_args=data\n",
    "    )\n",
    "    \n",
    "    row = []\n",
    "    row.extend(list(params.values()))\n",
    "    row.extend(list(train_stats.values()))\n",
    "    row.extend(list(eval_stats.values()))\n",
    "    \n",
    "    header = []\n",
    "    header.extend(list(params.keys()))\n",
    "    header.extend(list(train_stats.keys()))\n",
    "    header.extend(list(eval_stats.keys()))\n",
    "\n",
    "    output_df = pd.DataFrame([row], columns=header)\n",
    "    \n",
    "    del model\n",
    "    del data\n",
    "    del training\n",
    "    del adapter\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb130ac9",
   "metadata": {
    "id": "FZR2TOk1hZyT"
   },
   "source": [
    "**Define Dictionary of Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3030f-43e4-4063-a922-a4376ed66743",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks = [\n",
    "    #\"cola\",\n",
    "    #\"mnli\",\n",
    "    #\"mrpc\",\n",
    "    #\"qnli\",\n",
    "    #\"qqp\",\n",
    "    \"rte\",\n",
    "    \"sst2\",\n",
    "    \"stsb\",\n",
    "    \"wnli\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816a7b2f",
   "metadata": {
    "executionInfo": {
     "elapsed": 157,
     "status": "ok",
     "timestamp": 1627331583420,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "F1FN14CdVqFP"
   },
   "outputs": [],
   "source": [
    "task = 'cola'\n",
    "paramDictionary = {\n",
    "    'task_name':[task],\n",
    "    'model_name_or_path':['roberta-base'],\n",
    "    'max_seq_length':[64, 128, 256],\n",
    "    'pad_to_max_length':[True],\n",
    "    'per_device_train_batch_size':[16, 32, 64],\n",
    "    'adam_beta1':[.9],\n",
    "    'adam_beta2':[.999],\n",
    "    'adam_epsilon':[1e-8,1e-7,1e-6],\n",
    "    'fp16':[True],\n",
    "    'learning_rate':[1e-5,5e-5,1e-4,5e-4,1e-3],\n",
    "    'warmup_ratio':[0.0],\n",
    "    'warmup_steps':[0],\n",
    "    'weight_decay':[0.0],\n",
    "    'do_train':[True],\n",
    "    'do_eval':[True],\n",
    "    'num_train_epochs':[10],\n",
    "    'overwrite_output_dir':[True],\n",
    "    'adapter_config':['pfeiffer']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221eee26",
   "metadata": {
    "id": "CHmh6eeThiTm"
   },
   "source": [
    "**Begin Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2372beb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "beab7a6853464a2fb9da17c3a0c53f80",
      "06e9193555754f3c820fb47c9322925d",
      "d9c12bc752e745149f3d3e0edf93fe10",
      "8d12e917320e4de7ad3bd165a4f991bf",
      "7f5b43aafb754ccd82c11b0f7fa6b210",
      "2d64c717a1354cefab8b3c9e6334577e",
      "99a900f85059444581e93cb6ec686c4d",
      "3c3c6c0c806e417db227ad8afe80335e",
      "ccac7134e00a4f6581d80ca2943a0fcf",
      "3ee4cb3cecf0451f925dc34ed709d2a0",
      "c8bbed77890e4deb9d9263c95310a42a",
      "0a67106bdf5740baa70ccdced2ddc14e",
      "7ec6b5f9910f4a6da32473732eb96508",
      "5a12798be0d44c5fbbcfff260f366b68",
      "0e0ea8ae0cca4ab0914bb608a721aa18",
      "c00d73694fdb483d9d1f2a0e4334e890",
      "7a4a19a374ca47e5ace3a71fef6f4179",
      "bdea7284db604e26af00969e632fe1d3",
      "bcd6ce6c84574455ba3dd425e2267241",
      "310079aaacc443a98064000f342261a8",
      "a8915cd5793a408092c6d0e7357c6ba1",
      "83aa40a78b5e4f5593190fb8751f1f21"
     ]
    },
    "executionInfo": {
     "elapsed": 885367,
     "status": "ok",
     "timestamp": 1627335786995,
     "user": {
      "displayName": "Snow Cones",
      "photoUrl": "",
      "userId": "05757556542173404456"
     },
     "user_tz": 240
    },
    "id": "uEcrWpiJbyed",
    "outputId": "f517ea12-a23e-4b6e-bd83-62614f8acf1c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/27/2021 09:39:38 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "07/27/2021 09:39:38 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-07,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/qnli/runs/Jul27_09-39-38_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/27/2021 09:39:39 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"qnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'qnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'qnli'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe52e5df9ebe48d29a80857d4aa4565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03340a34259248fdb1cb53de8986799b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd79c1869c9489489a8cd64fdf5da02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 09:39:44 - INFO - task -   Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 2], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
      "07/27/2021 09:39:44 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
      "07/27/2021 09:39:44 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running training *****\n",
      "  Num examples = 104743\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32740' max='32740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32740/32740 22:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.487700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.328100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.277800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.266900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.193700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.194100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.191000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.195100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.144600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.105200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.100600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.095000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-16000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-16000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-16500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-17000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-17500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-18000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-18500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-19000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-19500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-20000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-20000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-20500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-20500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-21000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-21000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-21500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-22000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-22500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-23000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-23500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-24000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-24500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-25000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-25000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-25500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-25500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-26000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-26500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-26500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-27000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-27000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-27500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-28000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-28500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-29000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-29500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-29500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-30000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-30500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-31000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-31000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-31500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-32000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-32500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-32500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/qnli\n",
      "Configuration saved in ./adapter/task/qnli/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/special_tokens_map.json\n",
      "07/27/2021 10:01:47 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5463\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='683' max='683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [683/683 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 10:01:53 - INFO - /home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/datasets/metric.py -   Removing /home/jason/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
      "07/27/2021 10:01:53 - INFO - task -   ***** Eval results qnli *****\n",
      "07/27/2021 10:01:53 - INFO - task -     eval_loss = 0.3384419083595276\n",
      "07/27/2021 10:01:53 - INFO - task -     eval_accuracy = 0.9114039904814205\n",
      "07/27/2021 10:01:53 - INFO - task -     eval_runtime = 5.2842\n",
      "07/27/2021 10:01:53 - INFO - task -     eval_samples_per_second = 1033.831\n",
      "07/27/2021 10:01:53 - INFO - task -     eval_steps_per_second = 129.253\n",
      "07/27/2021 10:01:53 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/27/2021 10:01:53 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "07/27/2021 10:01:53 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/qnli/runs/Jul27_10-01-53_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/27/2021 10:01:54 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"qnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'qnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'qnli'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b27d2649e364ef3a0481aea8fba2cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d521074ce6dc49bca74af9ec5744a97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8a184450994d92b49652e5aa5ce773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 10:01:59 - INFO - task -   Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 996, 6, 151, 42669, 43, 58, 35578, 13, 5, 221, 9788, 361, 212, 2938, 826, 18, 130, 12, 4862, 1657, 196, 9689, 21163, 13767, 8893, 23, 5, 9846, 9, 732, 366, 179, 23895, 13878, 6, 53, 51, 2312, 7, 5111, 223, 1754, 3177, 8, 1577, 8848, 323, 668, 578, 41324, 19, 103, 379, 6, 151, 6981, 12675, 4, 2, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
      "07/27/2021 10:01:59 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
      "07/27/2021 10:01:59 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running training *****\n",
      "  Num examples = 104743\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 32740\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32740' max='32740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32740/32740 38:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.196800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.202900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.145800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.108200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.107400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.099200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-16000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-16000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-16500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-16500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-17000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-17000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-17500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-17500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-17500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-17500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-18000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-18000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-18500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-18500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-18500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-18500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-19000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-19000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-19500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-19500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-19500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-19500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-20000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-20000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-20500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-20500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-20500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-20500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-21000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-21000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-21500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-21500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-21500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-21500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-22000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-22000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-22500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-22500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-22500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-22500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-23000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-23000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-23500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-23500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-23500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-23500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-24000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-24000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-24500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-24500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-24500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-24500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-25000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-25000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-25500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-25500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-25500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-25500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-26000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-26000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-26500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-26500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-26500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-26500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-27000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-27000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-27500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-27500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-27500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-27500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-28000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-28000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-28500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-28500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-28500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-28500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-29000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-29000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-29500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-29500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-29500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-29500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-30000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-30000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-30500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-30500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-30500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-30500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-31000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-31000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-31500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-31500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-31500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-31500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-32000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-32000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-32500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-32500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-32500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-32500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/qnli\n",
      "Configuration saved in ./adapter/task/qnli/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/special_tokens_map.json\n",
      "07/27/2021 10:40:52 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5463\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='683' max='683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [683/683 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 10:41:01 - INFO - /home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/datasets/metric.py -   Removing /home/jason/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
      "07/27/2021 10:41:01 - INFO - task -   ***** Eval results qnli *****\n",
      "07/27/2021 10:41:01 - INFO - task -     eval_loss = 0.27380824089050293\n",
      "07/27/2021 10:41:01 - INFO - task -     eval_accuracy = 0.9229361156873512\n",
      "07/27/2021 10:41:01 - INFO - task -     eval_runtime = 8.5349\n",
      "07/27/2021 10:41:01 - INFO - task -     eval_samples_per_second = 640.079\n",
      "07/27/2021 10:41:01 - INFO - task -     eval_steps_per_second = 80.024\n",
      "07/27/2021 10:41:01 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/27/2021 10:41:01 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "07/27/2021 10:41:01 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-06,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/qnli/runs/Jul27_10-41-01_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/27/2021 10:41:01 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"qnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'qnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'qnli'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9739356e0f4617871b12872c9176cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=105.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c463c077ebe4745928ee57968677aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20e640123c10402a85a612bae9af7b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 10:41:07 - INFO - task -   Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 996, 6, 151, 42669, 43, 58, 35578, 13, 5, 221, 9788, 361, 212, 2938, 826, 18, 130, 12, 4862, 1657, 196, 9689, 21163, 13767, 8893, 23, 5, 9846, 9, 732, 366, 179, 23895, 13878, 6, 53, 51, 2312, 7, 5111, 223, 1754, 3177, 8, 1577, 8848, 323, 668, 578, 41324, 19, 103, 379, 6, 151, 6981, 12675, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
      "07/27/2021 10:41:07 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
      "07/27/2021 10:41:07 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running training *****\n",
      "  Num examples = 104743\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 16370\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16370' max='16370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16370/16370 1:16:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.323800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.217700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.135100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.092900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.096800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-16000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-16000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-16000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-16000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./adapter/task/qnli\n",
      "Configuration saved in ./adapter/task/qnli/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/special_tokens_map.json\n",
      "07/27/2021 11:57:31 - INFO - task -   *** Evaluate ***\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5463\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='683' max='683' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [683/683 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 11:57:48 - INFO - /home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/datasets/metric.py -   Removing /home/jason/.cache/huggingface/metrics/glue/qnli/default_experiment-1-0.arrow\n",
      "07/27/2021 11:57:48 - INFO - task -   ***** Eval results qnli *****\n",
      "07/27/2021 11:57:48 - INFO - task -     eval_loss = 0.2628665268421173\n",
      "07/27/2021 11:57:48 - INFO - task -     eval_accuracy = 0.9234852645066813\n",
      "07/27/2021 11:57:48 - INFO - task -     eval_runtime = 16.9653\n",
      "07/27/2021 11:57:48 - INFO - task -     eval_samples_per_second = 322.01\n",
      "07/27/2021 11:57:48 - INFO - task -     eval_steps_per_second = 40.259\n",
      "07/27/2021 11:57:48 - INFO - task -     epoch = 10.0\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "07/27/2021 11:57:48 - WARNING - task -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "07/27/2021 11:57:48 - INFO - task -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-07,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/task/qnli/runs/Jul27_11-57-48_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/task/qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/task/qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/27/2021 11:57:49 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"finetuning_task\": \"qnli\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding head 'qnli' with config {'head_type': 'classification', 'num_labels': 2, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'entailment': 0, 'not_entailment': 1}, 'use_pooler': False, 'bias': True}.\n",
      "Adding adapter 'qnli'.\n",
      "07/27/2021 11:57:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1ea2b63478b3fa54.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ea674fd58748ac82d3fc79e76324ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2021 11:57:51 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-c031cbf8f5df7c97.arrow\n",
      "07/27/2021 11:57:51 - INFO - task -   Sample 83810 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'idx': 83810, 'input_ids': [0, 2264, 8893, 9, 5, 221, 9788, 58, 382, 8, 248, 9335, 3517, 45, 2460, 7, 3679, 116, 2, 2, 4148, 974, 759, 23, 5, 2238, 4580, 760, 6, 10, 121, 4, 104, 4, 262, 212, 35614, 2925, 6304, 40955, 30456, 2711, 36, 246, 6, 151, 3878, 43, 8, 5, 121, 4, 104, 4, 112, 620, 6144, 2925, 36, 1092, 6, 151, 2383, 2], 'label': 0, 'question': 'What tactics of the PVA were US and ROK troops not prepared to handle?', 'sentence': \"On 27 November at the Korean eastern front, a U.S. 7th Infantry Division Regimental Combat Team (3,000 soldiers) and the U.S. 1st Marine Division (12,000–15,000 marines) were unprepared for the PVA 9th Army Group's three-pronged encirclement tactics at the Battle of Chosin Reservoir, but they managed to escape under Air Force and X Corps support fire—albeit with some 15,000 collective casualties.\"}.\n",
      "07/27/2021 11:57:51 - INFO - task -   Sample 14592 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 14592, 'input_ids': [0, 6179, 222, 5, 10377, 82, 619, 59, 234, 16151, 18, 1263, 7, 5, 908, 116, 2, 2, 487, 16151, 222, 45, 619, 14, 5, 10377, 2938, 21, 1227, 13, 10, 12065, 8, 222, 45, 28528, 16959, 19048, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 1, 'question': \"How did the Egyptian people feel about Nasser's response to the attack?\", 'sentence': 'Nasser did not feel that the Egyptian Army was ready for a confrontation and did not retaliate militarily.'}.\n",
      "07/27/2021 11:57:51 - INFO - task -   Sample 3278 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'idx': 3278, 'input_ids': [0, 2264, 761, 9, 3054, 32, 4453, 9, 602, 160, 32039, 116, 2, 2, 13863, 4062, 13565, 574, 3054, 32, 4453, 9, 602, 160, 32039, 31, 10, 1514, 15, 5, 9124, 6, 634, 5, 6854, 8, 10, 878, 386, 16, 444, 55, 2423, 5693, 8, 8621, 10, 19351, 1709, 2408, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'label': 0, 'question': 'What kind of aircraft are capable of taking off vertically?', 'sentence': 'Although STOVL aircraft are capable of taking off vertically from a spot on the deck, using the ramp and a running start is far more fuel efficient and permits a heavier launch weight.'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1052: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "The following columns in the training set  don't have a corresponding argument in `RobertaModelWithHeads.forward` and have been ignored: sentence, idx, question.\n",
      "***** Running training *****\n",
      "  Num examples = 104743\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15374' max='65470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15374/65470 06:19 < 20:37, 40.48 it/s, Epoch 2.35/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.533500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.392500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.330300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.302800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-1500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-1500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-1500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-2500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-2500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-2500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-3500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-3500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-3500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-4500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-4500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-4500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-4500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-5500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-5500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-5500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-6500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-6500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-6500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-7500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-7500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-7500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-7500/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-8500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-8500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-8500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-9500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-9500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-9500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10000/special_tokens_map.json\n",
      "/home/jason/.local/share/virtualenvs/cs7643-deep-learning-summer-2021-YD_jfhWv/lib/python3.8/site-packages/transformers/trainer.py:1351: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-10500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-10500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-10500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-11500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-11500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-11500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-11500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-12500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-12500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-12500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-12500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-13500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-13500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-13500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-13500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14000/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-14500\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-14500/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-14500/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-14500/special_tokens_map.json\n",
      "Saving model checkpoint to ./adapter/task/qnli/checkpoint-15000\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/adapter_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_adapter.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "Configuration saved in ./adapter/task/qnli/checkpoint-15000/qnli/head_config.json\n",
      "Module weights saved in ./adapter/task/qnli/checkpoint-15000/qnli/pytorch_model_head.bin\n",
      "tokenizer config file saved in ./adapter/task/qnli/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in ./adapter/task/qnli/checkpoint-15000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "limit = 15 #Numerical or False for no limit\n",
    "\n",
    "for data_set in glue_tasks:\n",
    "    paramDictionary[\"task_name\"] = [data_set]\n",
    "    paramsList = getParams(paramDictionary, limit)\n",
    "\n",
    "    results = None\n",
    "    for p in paramsList:\n",
    "        trial_data = train(p)\n",
    "        \n",
    "        if results is not None:\n",
    "            results = results.append(trial_data)\n",
    "        else:\n",
    "            results = trial_data #first pass through the loop\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "    results.to_csv(f\"./adapter/task/{data_set}_hp_search.{time():.0f}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88958964-0df8-4786-b69b-61d702e2d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83b4d8a3-2f84-4dde-ab1c-432319390c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f\"./adapter/task/{data_set}_hp_search.{time():.0f}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c4347-657e-42ed-b2b3-391dafb5f392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task_adapter_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06e9193555754f3c820fb47c9322925d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a67106bdf5740baa70ccdced2ddc14e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a12798be0d44c5fbbcfff260f366b68",
       "IPY_MODEL_0e0ea8ae0cca4ab0914bb608a721aa18",
       "IPY_MODEL_c00d73694fdb483d9d1f2a0e4334e890"
      ],
      "layout": "IPY_MODEL_7ec6b5f9910f4a6da32473732eb96508"
     }
    },
    "0e0ea8ae0cca4ab0914bb608a721aa18": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310079aaacc443a98064000f342261a8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bcd6ce6c84574455ba3dd425e2267241",
      "value": 2
     }
    },
    "2d64c717a1354cefab8b3c9e6334577e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "310079aaacc443a98064000f342261a8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3c6c0c806e417db227ad8afe80335e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ee4cb3cecf0451f925dc34ed709d2a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a12798be0d44c5fbbcfff260f366b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdea7284db604e26af00969e632fe1d3",
      "placeholder": "​",
      "style": "IPY_MODEL_7a4a19a374ca47e5ace3a71fef6f4179",
      "value": "100%"
     }
    },
    "7a4a19a374ca47e5ace3a71fef6f4179": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec6b5f9910f4a6da32473732eb96508": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f5b43aafb754ccd82c11b0f7fa6b210": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8bbed77890e4deb9d9263c95310a42a",
      "placeholder": "​",
      "style": "IPY_MODEL_3ee4cb3cecf0451f925dc34ed709d2a0",
      "value": " 2/2 [00:00&lt;00:00,  9.47ba/s]"
     }
    },
    "83aa40a78b5e4f5593190fb8751f1f21": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d12e917320e4de7ad3bd165a4f991bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccac7134e00a4f6581d80ca2943a0fcf",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c3c6c0c806e417db227ad8afe80335e",
      "value": 2
     }
    },
    "99a900f85059444581e93cb6ec686c4d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8915cd5793a408092c6d0e7357c6ba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bcd6ce6c84574455ba3dd425e2267241": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdea7284db604e26af00969e632fe1d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "beab7a6853464a2fb9da17c3a0c53f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9c12bc752e745149f3d3e0edf93fe10",
       "IPY_MODEL_8d12e917320e4de7ad3bd165a4f991bf",
       "IPY_MODEL_7f5b43aafb754ccd82c11b0f7fa6b210"
      ],
      "layout": "IPY_MODEL_06e9193555754f3c820fb47c9322925d"
     }
    },
    "c00d73694fdb483d9d1f2a0e4334e890": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83aa40a78b5e4f5593190fb8751f1f21",
      "placeholder": "​",
      "style": "IPY_MODEL_a8915cd5793a408092c6d0e7357c6ba1",
      "value": " 2/2 [00:00&lt;00:00,  6.16ba/s]"
     }
    },
    "c8bbed77890e4deb9d9263c95310a42a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccac7134e00a4f6581d80ca2943a0fcf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9c12bc752e745149f3d3e0edf93fe10": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99a900f85059444581e93cb6ec686c4d",
      "placeholder": "​",
      "style": "IPY_MODEL_2d64c717a1354cefab8b3c9e6334577e",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
