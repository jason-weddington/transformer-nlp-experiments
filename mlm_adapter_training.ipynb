{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8c4137-5f92-414e-bfc4-f066ea09e159",
   "metadata": {},
   "source": [
    "## Masked Language Modeling\n",
    "Using MLM, we train adapters for each of the GLUE tasks. This adapts the pre-trained language model to the language corpus specific to the GLUE task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32108f05-65f9-4708-b279-ad2a33100220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlm import masked_language_modeling\n",
    "from mlm_utils import DomainModelArguments, DomainDataTrainingArguments\n",
    "from transformers import TrainingArguments, MultiLingAdapterArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d797d3-e32f-4cd7-835f-b88c06ee7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks = [\n",
    "    \"cola\",\n",
    "    \"mnli\",\n",
    "    #\"mrpc\",\n",
    "    \"qnli\",\n",
    "    \"qqp\",\n",
    "    \"rte\",\n",
    "    \"sst2\",\n",
    "    \"stsb\",\n",
    "    \"wnli\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ec35e6-87d8-48e1-83b4-8e0bb0cfc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DomainModelArguments(\n",
    "    model_name_or_path=\"roberta-base\",\n",
    ")\n",
    "\n",
    "adapter = MultiLingAdapterArguments(\n",
    "    train_adapter=True,\n",
    "    adapter_config=\"pfeiffer+inv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79eb9949-5ca7-40b5-bbff-8ca186dbe7d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2021 12:41:49 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 12:41:49 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/cola/runs/Jul26_12-41-49_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/cola,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=cola,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/cola,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/26/2021 12:41:50 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 12:41:50,365 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 12:41:50,368 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 12:41:50,486 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 12:41:50,623 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 12:41:50,626 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,310 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,312 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,313 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,314 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,315 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:41:51,316 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 12:41:51,459 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 12:41:52,244 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 12:41:52,245 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 12:41:52,249 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ec10cef38504e666.arrow\n",
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-88a93ce67a4e181d.arrow\n",
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-231f91ea1ab5bd4f.arrow\n",
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cf457f7d064846e8.arrow\n",
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-012200867ffd1987.arrow\n",
      "07/26/2021 12:41:52 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-955c7ae95bba23dc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 12:41:54,411 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 12:41:54,416 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 12:41:54,416 >>   Num examples = 185\n",
      "[INFO|trainer.py:1201] 2021-07-26 12:41:54,416 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 12:41:54,416 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 12:41:54,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 12:41:54,417 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 12:41:54,417 >>   Total optimization steps = 240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-07-26 12:42:31,384 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:42:31,385 >> Saving model checkpoint to ./adapter/mlm/cola\n",
      "[INFO|loading.py:59] 2021-07-26 12:42:31,386 >> Configuration saved in ./adapter/mlm/cola/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:42:31,393 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:42:31,394 >> Configuration saved in ./adapter/mlm/cola/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:42:31,560 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:42:31,561 >> Configuration saved in ./adapter/mlm/cola/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:42:31,765 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:42:31,765 >> tokenizer config file saved in ./adapter/mlm/cola/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:42:31,766 >> Special tokens file saved in ./adapter/mlm/cola/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =   666311GF\n",
      "  train_loss               =       1.25\n",
      "  train_runtime            = 0:00:36.96\n",
      "  train_samples            =        185\n",
      "  train_samples_per_second =     50.044\n",
      "  train_steps_per_second   =      6.492\n",
      "07/26/2021 12:42:31 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 12:42:31,820 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 12:42:31,821 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 12:42:31,821 >>   Num examples = 22\n",
      "[INFO|trainer.py:2244] 2021-07-26 12:42:31,821 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.6797\n",
      "  eval_runtime            = 0:00:00.19\n",
      "  eval_samples            =         22\n",
      "  eval_samples_per_second =    112.484\n",
      "  eval_steps_per_second   =     15.339\n",
      "  perplexity              =      5.364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 12:42:32,021 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 12:42:32,022 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2021 12:42:32 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 12:42:32 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/mnli/runs/Jul26_12-42-32_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/mnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=mnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/mnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/26/2021 12:42:32 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "07/26/2021 12:42:33 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "07/26/2021 12:42:34 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 12:42:34,166 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 12:42:34,168 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 12:42:34,285 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 12:42:34,391 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 12:42:34,393 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,054 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,056 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,056 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,057 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,058 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 12:42:35,058 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 12:42:35,199 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 12:42:35,962 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 12:42:35,963 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 12:42:35,967 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7f18c10220db49b0.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8461d8948d16f90f.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-08a223d52a11606e.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2ac12d8d60febb91.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-10c26021640ebc7f.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9c1ff67b377fd7a4.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-0d3ccfbd51f702dc.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-de77f07414f4030d.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-111af11e6c8f185a.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-a5e21d5c2d33d063.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-7c827ef644b83fc3.arrow\n",
      "07/26/2021 12:42:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-39404c7ae77436b5.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 12:42:36,179 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 12:42:36,184 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 12:42:36,184 >>   Num examples = 19060\n",
      "[INFO|trainer.py:1201] 2021-07-26 12:42:36,185 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 12:42:36,185 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 12:42:36,185 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 12:42:36,185 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 12:42:36,185 >>   Total optimization steps = 23830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23830' max='23830' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23830/23830 1:03:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.107100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.994900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>2.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.988200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.961100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.963000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.943000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.958400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>1.952100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1989] 2021-07-26 12:43:56,367 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-500\n",
      "[INFO|loading.py:59] 2021-07-26 12:43:56,368 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:43:56,375 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:43:56,375 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:43:56,536 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:43:56,537 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:43:56,731 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:43:56,732 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:43:56,732 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:45:16,761 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-1000\n",
      "[INFO|loading.py:59] 2021-07-26 12:45:16,762 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:45:16,769 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:45:16,770 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:45:16,941 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:45:16,942 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:45:17,141 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:45:17,141 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:45:17,142 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:46:36,312 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-1500\n",
      "[INFO|loading.py:59] 2021-07-26 12:46:36,312 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:46:36,320 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:46:36,320 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:46:36,491 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:46:36,492 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:46:36,688 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:46:36,688 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:46:36,689 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:47:56,539 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-2000\n",
      "[INFO|loading.py:59] 2021-07-26 12:47:56,539 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:47:56,546 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:47:56,547 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:47:56,716 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:47:56,717 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:47:56,915 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:47:56,916 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:47:56,916 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:49:16,523 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-2500\n",
      "[INFO|loading.py:59] 2021-07-26 12:49:16,569 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:49:16,578 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:49:16,579 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:49:16,750 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:49:16,751 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:49:16,943 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:49:16,943 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:49:16,944 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:50:37,490 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-3000\n",
      "[INFO|loading.py:59] 2021-07-26 12:50:37,491 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:50:37,498 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:50:37,499 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:50:37,668 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:50:37,668 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:50:37,863 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:50:37,864 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:50:37,864 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:51:58,899 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-3500\n",
      "[INFO|loading.py:59] 2021-07-26 12:51:58,900 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:51:58,907 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:51:58,907 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:51:59,084 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:51:59,085 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:51:59,285 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:51:59,285 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:51:59,286 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:53:20,645 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-4000\n",
      "[INFO|loading.py:59] 2021-07-26 12:53:20,646 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:53:20,653 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:53:20,653 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:53:20,824 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:53:20,825 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:53:21,018 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:53:21,018 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:53:21,019 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:54:41,744 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-4500\n",
      "[INFO|loading.py:59] 2021-07-26 12:54:41,744 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:54:41,753 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:54:41,753 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:54:41,937 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:54:41,938 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-4500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:54:42,134 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-4500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:54:42,135 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:54:42,135 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:56:02,830 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-5000\n",
      "[INFO|loading.py:59] 2021-07-26 12:56:02,830 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:56:02,838 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:56:02,838 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:56:03,004 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:56:03,005 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:56:03,201 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:56:03,201 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:56:03,202 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:57:24,405 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-5500\n",
      "[INFO|loading.py:59] 2021-07-26 12:57:24,406 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:57:24,414 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:57:24,415 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:57:24,592 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:57:24,592 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-5500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:57:24,795 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-5500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:57:24,795 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:57:24,796 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 12:58:45,808 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-6000\n",
      "[INFO|loading.py:59] 2021-07-26 12:58:45,808 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:58:45,815 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:58:45,816 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:58:45,996 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 12:58:45,997 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 12:58:46,199 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 12:58:46,200 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 12:58:46,200 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:00:05,931 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-6500\n",
      "[INFO|loading.py:59] 2021-07-26 13:00:05,933 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:00:05,946 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:00:05,946 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:00:06,118 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:00:06,119 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-6500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:00:06,300 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-6500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:00:06,301 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:00:06,302 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:01:25,556 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-7000\n",
      "[INFO|loading.py:59] 2021-07-26 13:01:25,557 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:01:25,564 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:01:25,565 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:01:25,736 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:01:25,737 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:01:25,932 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:01:25,933 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:01:25,933 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:02:44,935 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-7500\n",
      "[INFO|loading.py:59] 2021-07-26 13:02:44,935 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:02:44,946 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:02:44,947 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:02:45,133 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:02:45,134 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-7500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:02:45,335 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-7500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:02:45,336 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:02:45,336 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:04:04,367 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-8000\n",
      "[INFO|loading.py:59] 2021-07-26 13:04:04,368 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:04:04,375 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:04:04,375 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:04:04,549 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:04:04,550 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:04:04,734 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:04:04,735 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:04:04,735 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:05:23,781 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-8500\n",
      "[INFO|loading.py:59] 2021-07-26 13:05:23,781 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:05:23,788 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:05:23,789 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:05:23,968 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:05:23,968 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-8500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:05:24,163 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-8500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:05:24,164 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:05:24,164 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:06:43,309 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-9000\n",
      "[INFO|loading.py:59] 2021-07-26 13:06:43,310 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:06:43,317 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:06:43,317 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:06:43,496 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:06:43,496 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:06:43,698 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:06:43,699 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:06:43,699 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:08:02,778 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-9500\n",
      "[INFO|loading.py:59] 2021-07-26 13:08:02,778 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:08:02,785 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:08:02,786 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:08:02,954 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:08:02,955 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-9500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:08:03,152 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-9500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:08:03,153 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:08:03,153 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:09:22,056 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-10000\n",
      "[INFO|loading.py:59] 2021-07-26 13:09:22,056 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:09:22,063 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:09:22,064 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:09:22,243 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:09:22,244 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:09:22,444 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:09:22,445 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:09:22,445 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:10:41,482 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-10500\n",
      "[INFO|loading.py:59] 2021-07-26 13:10:41,483 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:10:41,490 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:10:41,491 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:10:41,665 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:10:41,666 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-10500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:10:41,864 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-10500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:10:41,865 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:10:41,866 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:12:00,958 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-11000\n",
      "[INFO|loading.py:59] 2021-07-26 13:12:00,959 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:12:00,966 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:12:00,966 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:12:01,139 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:12:01,140 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:12:01,333 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:12:01,333 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:12:01,334 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:13:20,313 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-11500\n",
      "[INFO|loading.py:59] 2021-07-26 13:13:20,314 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:13:20,322 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:13:20,323 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:13:20,502 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:13:20,502 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-11500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:13:20,695 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-11500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:13:20,696 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:13:20,696 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:14:39,712 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-12000\n",
      "[INFO|loading.py:59] 2021-07-26 13:14:39,712 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:14:39,719 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:14:39,720 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:14:39,895 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:14:39,895 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:14:40,079 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:14:40,079 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:14:40,080 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:15:58,985 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-12500\n",
      "[INFO|loading.py:59] 2021-07-26 13:15:58,986 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:15:58,992 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:15:58,992 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:15:59,180 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:15:59,180 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-12500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:15:59,375 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-12500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:15:59,376 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:15:59,376 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:17:17,525 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-13000\n",
      "[INFO|loading.py:59] 2021-07-26 13:17:17,526 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:17:17,533 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:17:17,533 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:17:17,709 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:17:17,709 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:17:17,905 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:17:17,906 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:17:17,906 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-13000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:18:35,982 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-13500\n",
      "[INFO|loading.py:59] 2021-07-26 13:18:35,983 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:18:35,989 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:18:35,990 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:18:36,164 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:18:36,164 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-13500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:18:36,358 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-13500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:18:36,358 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:18:36,359 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-13500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:19:54,524 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-14000\n",
      "[INFO|loading.py:59] 2021-07-26 13:19:54,524 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:19:54,531 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:19:54,531 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:19:54,707 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:19:54,708 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:19:54,886 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:19:54,887 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:19:54,887 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-14000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:21:12,922 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-14500\n",
      "[INFO|loading.py:59] 2021-07-26 13:21:12,923 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:21:12,929 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:21:12,930 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:21:13,102 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:21:13,103 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-14500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:21:13,299 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-14500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:21:13,300 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:21:13,300 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-14500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:22:31,373 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-15000\n",
      "[INFO|loading.py:59] 2021-07-26 13:22:31,374 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:22:31,381 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:22:31,382 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:22:31,551 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:22:31,552 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:22:31,744 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:22:31,745 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:22:31,745 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-15000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:23:49,963 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-15500\n",
      "[INFO|loading.py:59] 2021-07-26 13:23:49,964 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:23:49,970 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:23:49,971 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:23:50,144 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:23:50,144 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-15500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:23:50,338 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-15500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:23:50,338 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:23:50,339 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-15500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:25:08,378 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-16000\n",
      "[INFO|loading.py:59] 2021-07-26 13:25:08,379 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:25:08,385 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:25:08,385 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:25:08,558 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:25:08,558 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:25:08,753 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:25:08,754 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:25:08,754 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-16000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:26:26,874 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-16500\n",
      "[INFO|loading.py:59] 2021-07-26 13:26:26,875 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:26:26,881 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:26:26,882 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:26:27,055 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:26:27,056 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-16500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:26:27,248 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-16500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:26:27,249 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:26:27,250 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-16500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:27:45,250 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-17000\n",
      "[INFO|loading.py:59] 2021-07-26 13:27:45,250 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:27:45,257 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:27:45,257 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:27:45,439 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:27:45,440 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:27:45,632 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:27:45,633 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:27:45,633 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-17000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:29:03,723 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-17500\n",
      "[INFO|loading.py:59] 2021-07-26 13:29:03,724 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:29:03,730 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:29:03,731 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:29:03,908 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:29:03,909 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-17500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:29:04,111 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-17500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:29:04,112 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:29:04,112 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-17500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:30:22,276 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-18000\n",
      "[INFO|loading.py:59] 2021-07-26 13:30:22,277 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:30:22,283 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:30:22,284 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:30:22,460 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:30:22,460 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:30:22,662 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:30:22,663 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:30:22,663 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-18000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:31:40,726 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-18500\n",
      "[INFO|loading.py:59] 2021-07-26 13:31:40,727 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:31:40,733 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:31:40,734 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:31:40,911 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:31:40,912 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-18500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:31:41,116 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-18500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:31:41,116 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:31:41,117 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-18500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:32:59,334 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-19000\n",
      "[INFO|loading.py:59] 2021-07-26 13:32:59,335 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:32:59,341 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:32:59,342 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:32:59,520 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:32:59,521 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:32:59,726 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:32:59,727 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:32:59,727 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-19000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:34:17,808 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-19500\n",
      "[INFO|loading.py:59] 2021-07-26 13:34:17,808 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:34:17,815 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:34:17,815 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:34:18,000 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:34:18,001 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-19500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:34:18,207 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-19500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:34:18,208 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:34:18,208 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-19500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:35:36,295 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-20000\n",
      "[INFO|loading.py:59] 2021-07-26 13:35:36,296 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:35:36,302 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:35:36,303 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:35:36,494 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:35:36,495 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:35:36,704 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:35:36,705 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:35:36,705 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-20000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:36:55,996 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-20500\n",
      "[INFO|loading.py:59] 2021-07-26 13:36:55,996 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:36:56,003 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:36:56,004 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:36:56,209 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:36:56,210 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-20500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:36:56,413 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-20500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:36:56,413 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:36:56,414 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-20500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:38:15,915 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-21000\n",
      "[INFO|loading.py:59] 2021-07-26 13:38:15,915 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:38:15,922 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:38:15,923 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:38:16,111 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:38:16,111 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:38:16,314 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:38:16,315 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:38:16,315 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-21000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:39:35,950 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-21500\n",
      "[INFO|loading.py:59] 2021-07-26 13:39:35,950 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:39:35,958 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:39:35,959 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:39:36,157 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:39:36,158 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-21500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:39:36,364 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-21500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:39:36,365 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:39:36,365 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-21500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:40:55,722 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-22000\n",
      "[INFO|loading.py:59] 2021-07-26 13:40:55,723 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:40:55,730 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:40:55,730 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:40:55,919 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:40:55,919 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:40:56,125 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:40:56,126 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:40:56,126 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-22000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:42:15,481 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-22500\n",
      "[INFO|loading.py:59] 2021-07-26 13:42:15,481 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:42:15,489 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:42:15,489 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:42:15,671 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:42:15,672 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-22500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:42:15,876 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-22500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:42:15,876 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:42:15,877 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-22500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:43:35,294 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-23000\n",
      "[INFO|loading.py:59] 2021-07-26 13:43:35,295 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:43:35,302 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:43:35,303 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:43:35,478 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:43:35,478 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:43:35,679 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:43:35,680 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:43:35,680 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-23000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:44:55,024 >> Saving model checkpoint to ./adapter/mlm/mnli/checkpoint-23500\n",
      "[INFO|loading.py:59] 2021-07-26 13:44:55,025 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:44:55,032 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:44:55,033 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:44:55,206 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:44:55,206 >> Configuration saved in ./adapter/mlm/mnli/checkpoint-23500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:44:55,400 >> Module weights saved in ./adapter/mlm/mnli/checkpoint-23500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:44:55,401 >> tokenizer config file saved in ./adapter/mlm/mnli/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:44:55,401 >> Special tokens file saved in ./adapter/mlm/mnli/checkpoint-23500/special_tokens_map.json\n",
      "[INFO|trainer.py:1403] 2021-07-26 13:45:47,664 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:45:47,665 >> Saving model checkpoint to ./adapter/mlm/mnli\n",
      "[INFO|loading.py:59] 2021-07-26 13:45:47,665 >> Configuration saved in ./adapter/mlm/mnli/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:45:47,673 >> Module weights saved in ./adapter/mlm/mnli/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:45:47,673 >> Configuration saved in ./adapter/mlm/mnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:45:47,850 >> Module weights saved in ./adapter/mlm/mnli/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:45:47,851 >> Configuration saved in ./adapter/mlm/mnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:45:48,048 >> Module weights saved in ./adapter/mlm/mnli/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:45:48,049 >> tokenizer config file saved in ./adapter/mlm/mnli/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:45:48,050 >> Special tokens file saved in ./adapter/mlm/mnli/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 68648121GF\n",
      "  train_loss               =     2.0062\n",
      "  train_runtime            = 1:03:11.47\n",
      "  train_samples            =      19060\n",
      "  train_samples_per_second =     50.271\n",
      "  train_steps_per_second   =      6.285\n",
      "07/26/2021 13:45:48 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 13:45:48,102 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 13:45:48,104 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 13:45:48,104 >>   Num examples = 994\n",
      "[INFO|trainer.py:2244] 2021-07-26 13:45:48,104 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 13:45:56,520 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 13:45:56,521 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.8546\n",
      "  eval_runtime            = 0:00:08.40\n",
      "  eval_samples            =        994\n",
      "  eval_samples_per_second =    118.286\n",
      "  eval_steps_per_second   =     14.875\n",
      "  perplexity              =     6.3893\n",
      "07/26/2021 13:45:56 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 13:45:56 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/qnli/runs/Jul26_13-45-56_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/qnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/qnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/qnli (download: 10.14 MiB, generated: 27.11 MiB, post-processed: Unknown size, total: 37.24 MiB) to /home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fc12c21b1e4c3eb19cf2e543ff79fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=10627589.0, style=ProgressStyle(descrip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 13:46:01,473 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 13:46:01,475 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 13:46:01,605 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 13:46:01,742 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 13:46:01,744 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,500 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,502 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,503 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,505 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,506 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:46:02,506 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 13:46:02,649 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 13:46:03,440 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 13:46:03,441 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 13:46:03,446 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f260003480234c2a90f9e232657fbacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=105.0, s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4543ad1602b245b8829074d5ee9459bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=6.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbe3a3efd3142c2a016245b2f0fa5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=6.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46e6ed12a744479a23e3bd57783dce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=105.0, style=Progre"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2952785662b409bbd2d3f91866e5cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=6.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53ac4ec60e446cba7320d62636a3af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=6.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 13:46:10,411 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 13:46:10,416 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 13:46:10,416 >>   Num examples = 2857\n",
      "[INFO|trainer.py:1201] 2021-07-26 13:46:10,416 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 13:46:10,416 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 13:46:10,416 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 13:46:10,417 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 13:46:10,417 >>   Total optimization steps = 3580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3580' max='3580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3580/3580 09:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.102100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1989] 2021-07-26 13:47:30,184 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-500\n",
      "[INFO|loading.py:59] 2021-07-26 13:47:30,184 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:47:30,192 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:47:30,192 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:47:30,359 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:47:30,359 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:47:30,552 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:47:30,553 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:47:30,553 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:48:49,920 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-1000\n",
      "[INFO|loading.py:59] 2021-07-26 13:48:49,920 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:48:49,927 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:48:49,928 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:48:50,101 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:48:50,102 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:48:50,296 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:48:50,297 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:48:50,297 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:50:08,901 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-1500\n",
      "[INFO|loading.py:59] 2021-07-26 13:50:08,901 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:50:08,909 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:50:08,909 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:50:09,086 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:50:09,086 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:50:09,279 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:50:09,280 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:50:09,280 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:51:28,423 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-2000\n",
      "[INFO|loading.py:59] 2021-07-26 13:51:28,423 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:51:28,431 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:51:28,431 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:51:28,597 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:51:28,598 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:51:28,795 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:51:28,795 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:51:28,796 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:52:50,241 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-2500\n",
      "[INFO|loading.py:59] 2021-07-26 13:52:50,242 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:52:50,249 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:52:50,250 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:52:50,429 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:52:50,430 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:52:50,626 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:52:50,627 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:52:50,627 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:54:11,865 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-3000\n",
      "[INFO|loading.py:59] 2021-07-26 13:54:11,865 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:54:11,873 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:54:11,873 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:54:12,049 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:54:12,050 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:54:12,248 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:54:12,248 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:54:12,249 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:55:34,375 >> Saving model checkpoint to ./adapter/mlm/qnli/checkpoint-3500\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:34,375 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:34,382 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:34,383 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:34,562 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:34,562 >> Configuration saved in ./adapter/mlm/qnli/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:34,763 >> Module weights saved in ./adapter/mlm/qnli/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:55:34,763 >> tokenizer config file saved in ./adapter/mlm/qnli/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:55:34,764 >> Special tokens file saved in ./adapter/mlm/qnli/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:1403] 2021-07-26 13:55:47,684 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:55:47,685 >> Saving model checkpoint to ./adapter/mlm/qnli\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:47,686 >> Configuration saved in ./adapter/mlm/qnli/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:47,692 >> Module weights saved in ./adapter/mlm/qnli/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:47,693 >> Configuration saved in ./adapter/mlm/qnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:47,870 >> Module weights saved in ./adapter/mlm/qnli/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:55:47,870 >> Configuration saved in ./adapter/mlm/qnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:55:48,076 >> Module weights saved in ./adapter/mlm/qnli/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:55:48,077 >> tokenizer config file saved in ./adapter/mlm/qnli/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:55:48,077 >> Special tokens file saved in ./adapter/mlm/qnli/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 10290014GF\n",
      "  train_loss               =     2.1544\n",
      "  train_runtime            = 0:09:37.26\n",
      "  train_samples            =       2857\n",
      "  train_samples_per_second =     49.492\n",
      "  train_steps_per_second   =      6.202\n",
      "07/26/2021 13:55:48 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 13:55:48,130 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 13:55:48,132 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 13:55:48,132 >>   Num examples = 150\n",
      "[INFO|trainer.py:2244] 2021-07-26 13:55:48,132 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 13:55:49,404 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 13:55:49,405 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     2.0639\n",
      "  eval_runtime            = 0:00:01.26\n",
      "  eval_samples            =        150\n",
      "  eval_samples_per_second =    118.551\n",
      "  eval_steps_per_second   =     15.016\n",
      "  perplexity              =     7.8769\n",
      "07/26/2021 13:55:49 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 13:55:49 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/qqp/runs/Jul26_13-55-49_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/qqp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=qqp,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/qqp,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /home/jason/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da5b1d168a847a595eed7186b0435a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=41696084.0, style=ProgressStyle(descrip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 13:56:04,732 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 13:56:04,735 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 13:56:04,842 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 13:56:04,960 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 13:56:04,962 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,643 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,645 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,646 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,647 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,648 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 13:56:05,649 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 13:56:05,814 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 13:56:06,578 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 13:56:06,579 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 13:56:06,584 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6977d1c1f3422980825918732b20a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=364.0, s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d771016f6df04b5fb1d6ac5157d5df12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=41.0, st"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a2f73c643a4cf198a5503be0bdec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=391.0, s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09e4993bd7d4a868df904e86e730aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=364.0, style=Progre"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857b5c0dd0074cca8b3b1b0947e82dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=41.0, style=Progres"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65c3c14c49e4c5abe80f231ed8d4703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=391.0, style=Progre"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 13:56:55,801 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 13:56:55,807 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 13:56:55,808 >>   Num examples = 10788\n",
      "[INFO|trainer.py:1201] 2021-07-26 13:56:55,808 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 13:56:55,809 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 13:56:55,809 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 13:56:55,809 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 13:56:55,809 >>   Total optimization steps = 13490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13490' max='13490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13490/13490 36:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.779000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.714300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.669900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.663000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.648000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.652200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.641900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1989] 2021-07-26 13:58:15,888 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-500\n",
      "[INFO|loading.py:59] 2021-07-26 13:58:15,888 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:58:15,895 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:58:15,896 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:58:16,082 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:58:16,082 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:58:16,279 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:58:16,280 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:58:16,281 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 13:59:36,286 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-1000\n",
      "[INFO|loading.py:59] 2021-07-26 13:59:36,287 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:59:36,294 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:59:36,294 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:59:36,476 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 13:59:36,477 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 13:59:36,672 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 13:59:36,673 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 13:59:36,673 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:00:56,611 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-1500\n",
      "[INFO|loading.py:59] 2021-07-26 14:00:56,612 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:00:56,619 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:00:56,619 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:00:56,808 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:00:56,809 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:00:57,007 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:00:57,008 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:00:57,008 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:02:17,600 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-2000\n",
      "[INFO|loading.py:59] 2021-07-26 14:02:17,601 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:02:17,608 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:02:17,608 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:02:17,795 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:02:17,796 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:02:17,990 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:02:17,990 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:02:17,991 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:03:37,219 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-2500\n",
      "[INFO|loading.py:59] 2021-07-26 14:03:37,220 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:03:37,227 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:03:37,228 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:03:37,418 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:03:37,419 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-2500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:03:37,618 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-2500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:03:37,618 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:03:37,619 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-2500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:04:56,590 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-3000\n",
      "[INFO|loading.py:59] 2021-07-26 14:04:56,591 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:04:56,598 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:04:56,598 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:04:56,779 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:04:56,779 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:04:56,982 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:04:56,982 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:04:56,983 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-3000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:06:17,661 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-3500\n",
      "[INFO|loading.py:59] 2021-07-26 14:06:17,662 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:06:17,714 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:06:17,715 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:06:17,894 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:06:17,895 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-3500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:06:18,087 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-3500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:06:18,087 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:06:18,088 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-3500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:07:38,490 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-4000\n",
      "[INFO|loading.py:59] 2021-07-26 14:07:38,490 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:07:38,497 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:07:38,498 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:07:38,677 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:07:38,677 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:07:38,880 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:07:38,881 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:07:38,881 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-4000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:09:00,516 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-4500\n",
      "[INFO|loading.py:59] 2021-07-26 14:09:00,517 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:09:00,524 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:09:00,524 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:09:00,703 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:09:00,704 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-4500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:09:00,902 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-4500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:09:00,902 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:09:00,903 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-4500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:10:20,151 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-5000\n",
      "[INFO|loading.py:59] 2021-07-26 14:10:20,152 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:10:20,159 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:10:20,159 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:10:20,346 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:10:20,347 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:10:20,547 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:10:20,547 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:10:20,548 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-5000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:11:40,926 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-5500\n",
      "[INFO|loading.py:59] 2021-07-26 14:11:40,927 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:11:40,934 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:11:40,935 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:11:41,123 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:11:41,124 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-5500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:11:41,327 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-5500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:11:41,328 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:11:41,328 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-5500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:13:01,831 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-6000\n",
      "[INFO|loading.py:59] 2021-07-26 14:13:01,832 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:13:01,838 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:13:01,839 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:13:02,027 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:13:02,028 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:13:02,227 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:13:02,227 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:13:02,228 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-6000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:14:21,987 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-6500\n",
      "[INFO|loading.py:59] 2021-07-26 14:14:21,988 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:14:21,994 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:14:21,995 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:14:22,175 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:14:22,176 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-6500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:14:22,375 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-6500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:14:22,376 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:14:22,376 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-6500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:15:41,394 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-7000\n",
      "[INFO|loading.py:59] 2021-07-26 14:15:41,395 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:15:41,402 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:15:41,402 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:15:41,592 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:15:41,592 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:15:41,792 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:15:41,792 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:15:41,793 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-7000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:17:01,385 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-7500\n",
      "[INFO|loading.py:59] 2021-07-26 14:17:01,386 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:17:01,393 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:17:01,393 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:17:01,577 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:17:01,578 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-7500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:17:01,782 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-7500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:17:01,783 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:17:01,783 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-7500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:18:21,260 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-8000\n",
      "[INFO|loading.py:59] 2021-07-26 14:18:21,261 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:18:21,312 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:18:21,313 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:18:21,492 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:18:21,493 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:18:21,681 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:18:21,682 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:18:21,682 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-8000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:19:41,134 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-8500\n",
      "[INFO|loading.py:59] 2021-07-26 14:19:41,134 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:19:41,141 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:19:41,142 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:19:41,325 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:19:41,326 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-8500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:19:41,528 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-8500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:19:41,529 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:19:41,529 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-8500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:21:01,015 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-9000\n",
      "[INFO|loading.py:59] 2021-07-26 14:21:01,016 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:21:01,023 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:21:01,024 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:21:01,205 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:21:01,206 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:21:01,410 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:21:01,410 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:21:01,411 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-9000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:22:22,579 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-9500\n",
      "[INFO|loading.py:59] 2021-07-26 14:22:22,580 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:22:22,587 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:22:22,588 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:22:22,770 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:22:22,771 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-9500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:22:22,978 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-9500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:22:22,979 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:22:22,979 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-9500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:23:46,289 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-10000\n",
      "[INFO|loading.py:59] 2021-07-26 14:23:46,289 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:23:46,296 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:23:46,297 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:23:46,486 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:23:46,486 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:23:46,688 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:23:46,689 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:23:46,689 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-10000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:25:06,749 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-10500\n",
      "[INFO|loading.py:59] 2021-07-26 14:25:06,790 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:25:06,797 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:25:06,798 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:25:06,973 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:25:06,974 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-10500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:25:07,166 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-10500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:25:07,167 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:25:07,167 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-10500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:26:26,674 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-11000\n",
      "[INFO|loading.py:59] 2021-07-26 14:26:26,675 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:26:26,682 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:26:26,683 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:26:26,872 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:26:26,873 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:26:27,080 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:26:27,081 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:26:27,081 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-11000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:27:47,578 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-11500\n",
      "[INFO|loading.py:59] 2021-07-26 14:27:47,578 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:27:47,585 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:27:47,586 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:27:47,777 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:27:47,778 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-11500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:27:47,984 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-11500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:27:47,984 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:27:47,985 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-11500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:29:08,344 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-12000\n",
      "[INFO|loading.py:59] 2021-07-26 14:29:08,344 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:29:08,354 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:29:08,354 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:29:08,558 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:29:08,559 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:29:08,757 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:29:08,758 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:29:08,758 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-12000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:30:28,910 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-12500\n",
      "[INFO|loading.py:59] 2021-07-26 14:30:28,910 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:30:28,917 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:30:28,918 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:30:29,103 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:30:29,104 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-12500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:30:29,306 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-12500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:30:29,307 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:30:29,307 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-12500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:31:49,951 >> Saving model checkpoint to ./adapter/mlm/qqp/checkpoint-13000\n",
      "[INFO|loading.py:59] 2021-07-26 14:31:49,951 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-13000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:31:49,960 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-13000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:31:49,961 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-13000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:31:50,146 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-13000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:31:50,147 >> Configuration saved in ./adapter/mlm/qqp/checkpoint-13000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:31:50,330 >> Module weights saved in ./adapter/mlm/qqp/checkpoint-13000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:31:50,331 >> tokenizer config file saved in ./adapter/mlm/qqp/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:31:50,331 >> Special tokens file saved in ./adapter/mlm/qqp/checkpoint-13000/special_tokens_map.json\n",
      "[INFO|trainer.py:1403] 2021-07-26 14:33:08,006 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:33:08,007 >> Saving model checkpoint to ./adapter/mlm/qqp\n",
      "[INFO|loading.py:59] 2021-07-26 14:33:08,008 >> Configuration saved in ./adapter/mlm/qqp/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:33:08,015 >> Module weights saved in ./adapter/mlm/qqp/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:33:08,015 >> Configuration saved in ./adapter/mlm/qqp/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:33:08,202 >> Module weights saved in ./adapter/mlm/qqp/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:33:08,202 >> Configuration saved in ./adapter/mlm/qqp/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:33:08,401 >> Module weights saved in ./adapter/mlm/qqp/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:33:08,401 >> tokenizer config file saved in ./adapter/mlm/qqp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:33:08,401 >> Special tokens file saved in ./adapter/mlm/qqp/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 38854980GF\n",
      "  train_loss               =     1.7051\n",
      "  train_runtime            = 0:36:12.19\n",
      "  train_samples            =      10788\n",
      "  train_samples_per_second =     49.664\n",
      "  train_steps_per_second   =       6.21\n",
      "07/26/2021 14:33:08 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:33:08,453 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 14:33:08,455 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 14:33:08,455 >>   Num examples = 1199\n",
      "[INFO|trainer.py:2244] 2021-07-26 14:33:08,455 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 14:33:18,494 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 14:33:18,495 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.5639\n",
      "  eval_runtime            = 0:00:10.02\n",
      "  eval_samples            =       1199\n",
      "  eval_samples_per_second =    119.549\n",
      "  eval_steps_per_second   =     14.956\n",
      "  perplexity              =     4.7774\n",
      "07/26/2021 14:33:18 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 14:33:18 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/rte/runs/Jul26_14-33-18_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/rte,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=rte,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/rte,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/rte (download: 680.81 KiB, generated: 1.83 MiB, post-processed: Unknown size, total: 2.49 MiB) to /home/jason/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ca470739ff46d7b165df11377d4ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=697150.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 14:33:21,233 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:33:21,236 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 14:33:21,387 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 14:33:21,502 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:33:21,504 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,177 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,180 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,182 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,183 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,184 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:33:22,185 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 14:33:22,363 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 14:33:23,086 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 14:33:23,087 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 14:33:23,093 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbc68957f9040618f3bc4b31b328749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=3.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc27e41cedb44bd0886d54e489c791f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=1.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a0058c7b8343feaa46fd0fb252dab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=3.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29cdfcecfbd4b2093831ef566d4e796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=3.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6303de624be84ef688a0a44e9a13ee05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=1.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e138187996c47a59195292084690d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=3.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:33:24,374 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 14:33:24,378 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 14:33:24,379 >>   Num examples = 278\n",
      "[INFO|trainer.py:1201] 2021-07-26 14:33:24,379 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 14:33:24,379 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 14:33:24,379 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 14:33:24,380 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 14:33:24,380 >>   Total optimization steps = 350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 00:55, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-07-26 14:34:19,987 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:34:19,988 >> Saving model checkpoint to ./adapter/mlm/rte\n",
      "[INFO|loading.py:59] 2021-07-26 14:34:19,989 >> Configuration saved in ./adapter/mlm/rte/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:34:19,998 >> Module weights saved in ./adapter/mlm/rte/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:34:19,999 >> Configuration saved in ./adapter/mlm/rte/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:34:20,223 >> Module weights saved in ./adapter/mlm/rte/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:34:20,224 >> Configuration saved in ./adapter/mlm/rte/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:34:20,421 >> Module weights saved in ./adapter/mlm/rte/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:34:20,422 >> tokenizer config file saved in ./adapter/mlm/rte/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:34:20,422 >> Special tokens file saved in ./adapter/mlm/rte/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =  1001268GF\n",
      "  train_loss               =     1.5585\n",
      "  train_runtime            = 0:00:55.60\n",
      "  train_samples            =        278\n",
      "  train_samples_per_second =     49.993\n",
      "  train_steps_per_second   =      6.294\n",
      "07/26/2021 14:34:20 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:34:20,474 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 14:34:20,476 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 14:34:20,476 >>   Num examples = 30\n",
      "[INFO|trainer.py:2244] 2021-07-26 14:34:20,476 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 14:34:20,743 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 14:34:20,743 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.4394\n",
      "  eval_runtime            = 0:00:00.26\n",
      "  eval_samples            =         30\n",
      "  eval_samples_per_second =    115.337\n",
      "  eval_steps_per_second   =     15.378\n",
      "  perplexity              =     4.2183\n",
      "07/26/2021 14:34:20 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 14:34:20 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/sst2/runs/Jul26_14-34-20_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/sst2,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=sst2,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/sst2,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/jason/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59f98e9c06345ce91b303816c334b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=7439277.0, style=ProgressStyle(descript"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 14:34:24,464 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:34:24,467 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:427] 2021-07-26 14:34:24,583 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 14:34:24,714 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:34:24,716 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,439 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,441 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,442 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,443 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,444 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:34:25,445 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 14:34:25,584 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 14:34:26,284 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 14:34:26,285 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 14:34:26,289 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7285a08e0442a681a2871a7d588af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=68.0, st"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486bf6641d1841cfb8d6a316430ed347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=1.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b250cff0f245b39c1358f3738b8384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=2.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb7fa5ca583491d9d05d5e701b20a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=68.0, style=Progres"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa36c434edb4181933962b64cc431c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=1.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f2b31e366f4f97bebc4f15ef571e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=2.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:34:30,623 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 14:34:30,628 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 14:34:30,628 >>   Num examples = 1853\n",
      "[INFO|trainer.py:1201] 2021-07-26 14:34:30,628 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 14:34:30,628 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 14:34:30,629 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 14:34:30,629 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 14:34:30,629 >>   Total optimization steps = 2320\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2320' max='2320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2320/2320 06:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.128500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1989] 2021-07-26 14:35:49,733 >> Saving model checkpoint to ./adapter/mlm/sst2/checkpoint-500\n",
      "[INFO|loading.py:59] 2021-07-26 14:35:49,734 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:35:49,740 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:35:49,741 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:35:49,900 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:35:49,901 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:35:50,081 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:35:50,082 >> tokenizer config file saved in ./adapter/mlm/sst2/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:35:50,082 >> Special tokens file saved in ./adapter/mlm/sst2/checkpoint-500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:37:10,084 >> Saving model checkpoint to ./adapter/mlm/sst2/checkpoint-1000\n",
      "[INFO|loading.py:59] 2021-07-26 14:37:10,085 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:37:10,092 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:37:10,092 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:37:10,245 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:37:10,246 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:37:10,434 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:37:10,435 >> tokenizer config file saved in ./adapter/mlm/sst2/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:37:10,436 >> Special tokens file saved in ./adapter/mlm/sst2/checkpoint-1000/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:38:31,459 >> Saving model checkpoint to ./adapter/mlm/sst2/checkpoint-1500\n",
      "[INFO|loading.py:59] 2021-07-26 14:38:31,460 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1500/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:38:31,467 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1500/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:38:31,468 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:38:31,632 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:38:31,633 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-1500/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:38:31,832 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-1500/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:38:31,833 >> tokenizer config file saved in ./adapter/mlm/sst2/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:38:31,833 >> Special tokens file saved in ./adapter/mlm/sst2/checkpoint-1500/special_tokens_map.json\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:39:51,186 >> Saving model checkpoint to ./adapter/mlm/sst2/checkpoint-2000\n",
      "[INFO|loading.py:59] 2021-07-26 14:39:51,186 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-2000/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:39:51,193 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-2000/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:39:51,194 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:39:51,355 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:39:51,355 >> Configuration saved in ./adapter/mlm/sst2/checkpoint-2000/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:39:51,535 >> Module weights saved in ./adapter/mlm/sst2/checkpoint-2000/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:39:51,536 >> tokenizer config file saved in ./adapter/mlm/sst2/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:39:51,536 >> Special tokens file saved in ./adapter/mlm/sst2/checkpoint-2000/special_tokens_map.json\n",
      "[INFO|trainer.py:1403] 2021-07-26 14:40:42,146 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:40:42,147 >> Saving model checkpoint to ./adapter/mlm/sst2\n",
      "[INFO|loading.py:59] 2021-07-26 14:40:42,148 >> Configuration saved in ./adapter/mlm/sst2/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:40:42,155 >> Module weights saved in ./adapter/mlm/sst2/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:40:42,155 >> Configuration saved in ./adapter/mlm/sst2/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:40:42,318 >> Module weights saved in ./adapter/mlm/sst2/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:40:42,319 >> Configuration saved in ./adapter/mlm/sst2/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:40:42,502 >> Module weights saved in ./adapter/mlm/sst2/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:40:42,502 >> tokenizer config file saved in ./adapter/mlm/sst2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:40:42,503 >> Special tokens file saved in ./adapter/mlm/sst2/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =  6673922GF\n",
      "  train_loss               =     2.2219\n",
      "  train_runtime            = 0:06:11.51\n",
      "  train_samples            =       1853\n",
      "  train_samples_per_second =     49.877\n",
      "  train_steps_per_second   =      6.245\n",
      "07/26/2021 14:40:42 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:40:42,556 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 14:40:42,558 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 14:40:42,558 >>   Num examples = 44\n",
      "[INFO|trainer.py:2244] 2021-07-26 14:40:42,558 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 14:40:42,938 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 14:40:42,939 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.8202\n",
      "  eval_runtime            = 0:00:00.37\n",
      "  eval_samples            =         44\n",
      "  eval_samples_per_second =    117.726\n",
      "  eval_steps_per_second   =     16.054\n",
      "  perplexity              =     6.1728\n",
      "07/26/2021 14:40:42 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 14:40:42 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/stsb/runs/Jul26_14-40-42_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/stsb,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=stsb,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/stsb,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/stsb (download: 784.05 KiB, generated: 1.09 MiB, post-processed: Unknown size, total: 1.86 MiB) to /home/jason/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6049ac8c50487092905ef617904d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=802872.0, style=ProgressStyle(descripti"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/stsb/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 14:40:45,577 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:40:45,579 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-26 14:40:45,718 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 14:40:45,826 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:40:45,829 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,481 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,483 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,484 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,485 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,486 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:40:46,487 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 14:40:46,636 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 14:40:47,342 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 14:40:47,343 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 14:40:47,349 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a3ae72751143339eb60e4d53af7161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=6.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8febb658234cb89fd5f06e9635597c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=2.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910ca7801bb64263864563b3200fd671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=2.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168d59e3e77f42b49e2d689c58f0c14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=6.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e35f274661246e48c29104b3dbba672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=2.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761b6671b73b4f1ca89a080394afa466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=2.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:40:48,058 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 14:40:48,063 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 14:40:48,063 >>   Num examples = 159\n",
      "[INFO|trainer.py:1201] 2021-07-26 14:40:48,063 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 14:40:48,063 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 14:40:48,064 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 14:40:48,064 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 14:40:48,064 >>   Total optimization steps = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 00:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-07-26 14:41:19,709 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:41:19,710 >> Saving model checkpoint to ./adapter/mlm/stsb\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:19,711 >> Configuration saved in ./adapter/mlm/stsb/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:19,718 >> Module weights saved in ./adapter/mlm/stsb/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:19,719 >> Configuration saved in ./adapter/mlm/stsb/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:19,899 >> Module weights saved in ./adapter/mlm/stsb/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:19,900 >> Configuration saved in ./adapter/mlm/stsb/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:20,115 >> Module weights saved in ./adapter/mlm/stsb/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:41:20,115 >> tokenizer config file saved in ./adapter/mlm/stsb/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:41:20,116 >> Special tokens file saved in ./adapter/mlm/stsb/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =   572667GF\n",
      "  train_loss               =     2.0214\n",
      "  train_runtime            = 0:00:31.64\n",
      "  train_samples            =        159\n",
      "  train_samples_per_second =     50.245\n",
      "  train_steps_per_second   =       6.32\n",
      "07/26/2021 14:41:20 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:41:20,166 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 14:41:20,168 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 14:41:20,168 >>   Num examples = 47\n",
      "[INFO|trainer.py:2244] 2021-07-26 14:41:20,168 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-26 14:41:20,573 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-26 14:41:20,573 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.7703\n",
      "  eval_runtime            = 0:00:00.39\n",
      "  eval_samples            =         47\n",
      "  eval_samples_per_second =    117.834\n",
      "  eval_steps_per_second   =     15.043\n",
      "  perplexity              =     5.8725\n",
      "07/26/2021 14:41:20 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/26/2021 14:41:20 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/wnli/runs/Jul26_14-41-20_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/wnli,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=wnli,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/wnli,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Downloading and preparing dataset glue/wnli (download: 28.32 KiB, generated: 154.03 KiB, post-processed: Unknown size, total: 182.35 KiB) to /home/jason/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c4b5ad961046b89024b4d90b470d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28999.0, style=ProgressStyle(descriptio"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-26 14:41:22,471 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:41:22,474 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/jason/.cache/huggingface/datasets/glue/wnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:427] 2021-07-26 14:41:22,599 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-26 14:41:22,707 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-26 14:41:22,709 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,431 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,433 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,434 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,435 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,436 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-26 14:41:23,436 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-26 14:41:23,580 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-26 14:41:24,282 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-26 14:41:24,283 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-26 14:41:24,292 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9915e5e3447487cb87aaec9a1f81618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=1.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249168f2dcf24c049412e5b63c60dedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=1.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503a24ac44ad4a9888b445ad1c042f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running tokenizer on every text in dataset', max=1.0, sty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc622118b26947c398c0779632127acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=1.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8160f17b35cf4afd95b0ef321ac03105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=1.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c366ae62c054f2eb55393970e2f6cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Grouping texts in chunks of 512', max=1.0, style=Progress"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:41:24,543 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-26 14:41:24,547 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-26 14:41:24,547 >>   Num examples = 31\n",
      "[INFO|trainer.py:1201] 2021-07-26 14:41:24,548 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-26 14:41:24,548 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-26 14:41:24,548 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-26 14:41:24,548 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-26 14:41:24,548 >>   Total optimization steps = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-07-26 14:41:30,664 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-26 14:41:30,665 >> Saving model checkpoint to ./adapter/mlm/wnli\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:30,666 >> Configuration saved in ./adapter/mlm/wnli/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:30,673 >> Module weights saved in ./adapter/mlm/wnli/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:30,674 >> Configuration saved in ./adapter/mlm/wnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:30,824 >> Module weights saved in ./adapter/mlm/wnli/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-26 14:41:30,825 >> Configuration saved in ./adapter/mlm/wnli/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-26 14:41:31,018 >> Module weights saved in ./adapter/mlm/wnli/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-26 14:41:31,018 >> tokenizer config file saved in ./adapter/mlm/wnli/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-26 14:41:31,019 >> Special tokens file saved in ./adapter/mlm/wnli/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =   111652GF\n",
      "  train_loss               =     1.6329\n",
      "  train_runtime            = 0:00:06.11\n",
      "  train_samples            =         31\n",
      "  train_samples_per_second =     50.686\n",
      "  train_steps_per_second   =       6.54\n",
      "07/26/2021 14:41:31 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-26 14:41:31,069 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-26 14:41:31,071 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-26 14:41:31,071 >>   Num examples = 3\n",
      "[INFO|trainer.py:2244] 2021-07-26 14:41:31,071 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.6839\n",
      "  eval_runtime            = 0:00:00.02\n",
      "  eval_samples            =          3\n",
      "  eval_samples_per_second =    105.691\n",
      "  eval_steps_per_second   =      35.23\n",
      "  perplexity              =     5.3866\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "results = {}\n",
    "for dataset in glue_tasks:\n",
    "    data = DomainDataTrainingArguments(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=dataset,\n",
    "    )\n",
    "    \n",
    "    training = TrainingArguments(\n",
    "        learning_rate=1e-4,\n",
    "        overwrite_output_dir=True,\n",
    "        output_dir=f\"./adapter/mlm/{dataset}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        num_train_epochs=10,\n",
    "    )\n",
    "\n",
    "    train_stats, eval_stats = masked_language_modeling(\n",
    "        model_args=model, data_args=data, training_args=training, adapter_args=adapter\n",
    "    )\n",
    "    \n",
    "    results[dataset] = {\"training\" : train_stats, \"eval\" : eval_stats}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85bc1017-3f73-49c3-b188-0e7bd4971f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cola': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.679701566696167,\n",
      "                   'eval_runtime': 0.1956,\n",
      "                   'eval_samples': 22,\n",
      "                   'eval_samples_per_second': 112.484,\n",
      "                   'eval_steps_per_second': 15.339,\n",
      "                   'perplexity': 5.3639549494375895},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 715446823680000.0,\n",
      "                       'train_loss': 1.250021235148112,\n",
      "                       'train_runtime': 36.9675,\n",
      "                       'train_samples': 185,\n",
      "                       'train_samples_per_second': 50.044,\n",
      "                       'train_steps_per_second': 6.492}},\n",
      " 'mnli': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.854617953300476,\n",
      "                   'eval_runtime': 8.4033,\n",
      "                   'eval_samples': 994,\n",
      "                   'eval_samples_per_second': 118.286,\n",
      "                   'eval_steps_per_second': 14.875,\n",
      "                   'perplexity': 6.389256789805616},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 7.371035923968e+16,\n",
      "                       'train_loss': 2.006234924342019,\n",
      "                       'train_runtime': 3791.4784,\n",
      "                       'train_samples': 19060,\n",
      "                       'train_samples_per_second': 50.271,\n",
      "                       'train_steps_per_second': 6.285}},\n",
      " 'qnli': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 2.0639328956604004,\n",
      "                   'eval_runtime': 1.2653,\n",
      "                   'eval_samples': 150,\n",
      "                   'eval_samples_per_second': 118.551,\n",
      "                   'eval_steps_per_second': 15.016,\n",
      "                   'perplexity': 7.876887949993143},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 1.1048819325696e+16,\n",
      "                       'train_loss': 2.1543563160816386,\n",
      "                       'train_runtime': 577.2675,\n",
      "                       'train_samples': 2857,\n",
      "                       'train_samples_per_second': 49.492,\n",
      "                       'train_steps_per_second': 6.202}},\n",
      " 'qqp': {'eval': {'epoch': 10.0,\n",
      "                  'eval_loss': 1.5638868808746338,\n",
      "                  'eval_runtime': 10.0293,\n",
      "                  'eval_samples': 1199,\n",
      "                  'eval_samples_per_second': 119.549,\n",
      "                  'eval_steps_per_second': 14.956,\n",
      "                  'perplexity': 4.777354210803524},\n",
      "         'training': {'epoch': 10.0,\n",
      "                      'total_flos': 4.1720218020864e+16,\n",
      "                      'train_loss': 1.7051421598826098,\n",
      "                      'train_runtime': 2172.197,\n",
      "                      'train_samples': 10788,\n",
      "                      'train_samples_per_second': 49.664,\n",
      "                      'train_steps_per_second': 6.21}},\n",
      " 'rte': {'eval': {'epoch': 10.0,\n",
      "                  'eval_loss': 1.4394282102584839,\n",
      "                  'eval_runtime': 0.2601,\n",
      "                  'eval_samples': 30,\n",
      "                  'eval_samples_per_second': 115.337,\n",
      "                  'eval_steps_per_second': 15.378,\n",
      "                  'perplexity': 4.218283156259398},\n",
      "         'training': {'epoch': 10.0,\n",
      "                      'total_flos': 1075103875584000.0,\n",
      "                      'train_loss': 1.5585063825334822,\n",
      "                      'train_runtime': 55.6074,\n",
      "                      'train_samples': 278,\n",
      "                      'train_samples_per_second': 49.993,\n",
      "                      'train_steps_per_second': 6.294}},\n",
      " 'sst2': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.8201522827148438,\n",
      "                   'eval_runtime': 0.3737,\n",
      "                   'eval_samples': 44,\n",
      "                   'eval_samples_per_second': 117.726,\n",
      "                   'eval_steps_per_second': 16.054,\n",
      "                   'perplexity': 6.172798388810342},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 7166070077184000.0,\n",
      "                       'train_loss': 2.221930142106681,\n",
      "                       'train_runtime': 371.5174,\n",
      "                       'train_samples': 1853,\n",
      "                       'train_samples_per_second': 49.877,\n",
      "                       'train_steps_per_second': 6.245}},\n",
      " 'stsb': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.7702865600585938,\n",
      "                   'eval_runtime': 0.3989,\n",
      "                   'eval_samples': 47,\n",
      "                   'eval_samples_per_second': 117.834,\n",
      "                   'eval_steps_per_second': 15.043,\n",
      "                   'perplexity': 5.872535954536317},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 614897540352000.0,\n",
      "                       'train_loss': 2.0214387512207033,\n",
      "                       'train_runtime': 31.6452,\n",
      "                       'train_samples': 159,\n",
      "                       'train_samples_per_second': 50.245,\n",
      "                       'train_steps_per_second': 6.32}},\n",
      " 'wnli': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.6839203834533691,\n",
      "                   'eval_runtime': 0.0284,\n",
      "                   'eval_samples': 3,\n",
      "                   'eval_samples_per_second': 105.691,\n",
      "                   'eval_steps_per_second': 35.23,\n",
      "                   'perplexity': 5.386632294610009},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 119885683968000.0,\n",
      "                       'train_loss': 1.6329341888427735,\n",
      "                       'train_runtime': 6.116,\n",
      "                       'train_samples': 31,\n",
      "                       'train_samples_per_second': 50.686,\n",
      "                       'train_steps_per_second': 6.54}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95243c5-4de5-404c-90a6-8129f78a6bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
