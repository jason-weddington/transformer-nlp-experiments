{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8c4137-5f92-414e-bfc4-f066ea09e159",
   "metadata": {},
   "source": [
    "## Masked Language Modeling\n",
    "Using MLM, we train adapters for each of the GLUE tasks. This adapts the pre-trained language model to the language corpus specific to the GLUE task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32108f05-65f9-4708-b279-ad2a33100220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# !pip install -Uqq adapter-transformers datasets\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils.mlm import masked_language_modeling\n",
    "from utils.mlm_utils import DomainModelArguments, DomainDataTrainingArguments\n",
    "from transformers import TrainingArguments, MultiLingAdapterArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74d797d3-e32f-4cd7-835f-b88c06ee7f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_tasks = [\n",
    "    #\"cola\",\n",
    "    #\"mnli\",\n",
    "    \"mrpc\",\n",
    "    #\"qnli\",\n",
    "    #\"qqp\",\n",
    "    #\"rte\",\n",
    "    #\"sst2\",\n",
    "    #\"stsb\",\n",
    "    #\"wnli\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ec35e6-87d8-48e1-83b4-8e0bb0cfc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DomainModelArguments(\n",
    "    model_name_or_path=\"roberta-base\",\n",
    ")\n",
    "\n",
    "adapter = MultiLingAdapterArguments(\n",
    "    train_adapter=True,\n",
    "    adapter_config=\"pfeiffer+inv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79eb9949-5ca7-40b5-bbff-8ca186dbe7d9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-08-02 15:25:07,338 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-08-02 15:25:07,339 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "08/02/2021 15:25:07 - WARNING - utils.mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "08/02/2021 15:25:07 - INFO - utils.mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/mrpc/runs/Aug02_15-25-07_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/mrpc,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=mrpc,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/mrpc,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "08/02/2021 15:25:07 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "[INFO|configuration_utils.py:531] 2021-08-02 15:25:07,824 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-08-02 15:25:07,827 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-08-02 15:25:07,938 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-08-02 15:25:08,053 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-08-02 15:25:08,055 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,779 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,781 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,782 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,784 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,786 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-08-02 15:25:08,787 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-08-02 15:25:08,928 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-08-02 15:25:09,751 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-08-02 15:25:09,751 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-08-02 15:25:09,760 >> Adding adapter 'glue'.\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f19cb8db9935839f.arrow\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b8a7e5b77c59585c.arrow\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-8e1604300b63939c.arrow\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5a9c2df4956a45b8.arrow\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-0b23e6506c0694f6.arrow\n",
      "08/02/2021 15:25:09 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9fb8bf34894bd372.arrow\n",
      "[INFO|trainer.py:546] 2021-08-02 15:25:09,905 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-08-02 15:25:09,909 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-08-02 15:25:09,909 >>   Num examples = 191\n",
      "[INFO|trainer.py:1201] 2021-08-02 15:25:09,910 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-08-02 15:25:09,910 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-08-02 15:25:09,910 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-08-02 15:25:09,910 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-08-02 15:25:09,910 >>   Total optimization steps = 240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-08-02 15:25:48,306 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-08-02 15:25:48,307 >> Saving model checkpoint to ./adapter/mlm/mrpc\n",
      "[INFO|loading.py:59] 2021-08-02 15:25:48,308 >> Configuration saved in ./adapter/mlm/mrpc/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:25:48,359 >> Module weights saved in ./adapter/mlm/mrpc/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:25:48,359 >> Configuration saved in ./adapter/mlm/mrpc/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:25:48,528 >> Module weights saved in ./adapter/mlm/mrpc/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-08-02 15:25:48,529 >> Configuration saved in ./adapter/mlm/mrpc/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-08-02 15:25:48,735 >> Module weights saved in ./adapter/mlm/mrpc/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-08-02 15:25:48,736 >> tokenizer config file saved in ./adapter/mlm/mrpc/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-08-02 15:25:48,737 >> Special tokens file saved in ./adapter/mlm/mrpc/special_tokens_map.json\n",
      "08/02/2021 15:25:48 - INFO - utils.mlm -   *** Evaluate ***\n",
      "[INFO|trainer.py:546] 2021-08-02 15:25:48,809 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-08-02 15:25:48,811 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-08-02 15:25:48,811 >>   Num examples = 21\n",
      "[INFO|trainer.py:2244] 2021-08-02 15:25:48,811 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =   687921GF\n",
      "  train_loss               =     1.8425\n",
      "  train_runtime            = 0:00:38.39\n",
      "  train_samples            =        191\n",
      "  train_samples_per_second =     49.745\n",
      "  train_steps_per_second   =      6.251\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.6364\n",
      "  eval_runtime            = 0:00:00.18\n",
      "  eval_samples            =         21\n",
      "  eval_samples_per_second =    114.161\n",
      "  eval_steps_per_second   =     16.309\n",
      "  perplexity              =     5.1365\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "results = {}\n",
    "for dataset in glue_tasks[:1]:\n",
    "    data = DomainDataTrainingArguments(\n",
    "        dataset_name=\"glue\",\n",
    "        dataset_config_name=dataset,\n",
    "    )\n",
    "    \n",
    "    training = TrainingArguments(\n",
    "        learning_rate=1e-4,\n",
    "        overwrite_output_dir=True,\n",
    "        output_dir=f\"./adapter/mlm/{dataset}\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        num_train_epochs=10,\n",
    "    )\n",
    "\n",
    "    train_stats, eval_stats = masked_language_modeling(\n",
    "        model_args=model, data_args=data, training_args=training, adapter_args=adapter\n",
    "    )\n",
    "    \n",
    "    results[dataset] = {\"training\" : train_stats, \"eval\" : eval_stats}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85bc1017-3f73-49c3-b188-0e7bd4971f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mrpc': {'eval': {'epoch': 10.0,\n",
      "                   'eval_loss': 1.636365294456482,\n",
      "                   'eval_runtime': 0.184,\n",
      "                   'eval_samples': 21,\n",
      "                   'eval_samples_per_second': 114.161,\n",
      "                   'eval_steps_per_second': 16.309,\n",
      "                   'perplexity': 5.136466000500812},\n",
      "          'training': {'epoch': 10.0,\n",
      "                       'total_flos': 738650504448000.0,\n",
      "                       'train_loss': 1.8425042470296225,\n",
      "                       'train_runtime': 38.3961,\n",
      "                       'train_samples': 191,\n",
      "                       'train_samples_per_second': 49.745,\n",
      "                       'train_steps_per_second': 6.251}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
