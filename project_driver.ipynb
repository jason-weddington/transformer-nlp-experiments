{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ece290d-c74d-49b0-9aaa-2655fd2364bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc193b1-918d-41c5-9cbf-82c1b63a4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport adapter_utils\n",
    "from adapter_utils import load_model_and_tokenizer, adapt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca3eee-831e-4c87-867f-ab777ba2aac9",
   "metadata": {},
   "source": [
    "Load and adapt a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c001bc77-84d4-427b-ab10-59fc36004197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_name=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82aed052-76b4-477b-9766-696bbe8d06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_model(model=model, adapter_name=\"qa/squad1@ukp\", adapter_arch=\"houlsby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9eb656-b483-4b9e-b334-aea5892b1064",
   "metadata": {},
   "source": [
    "Test that things are working by running the example from Adapter Hub's sample notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7550802-83ac-487d-95d6-021490d2691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import QuestionAnsweringPipeline\n",
    "\n",
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "context = \"\"\"\n",
    "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.\n",
    "Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.\n",
    "Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model.\n",
    "However, sharing and integrating adapter layers is not straightforward.\n",
    "We propose AdapterHub, a framework that allows dynamic \"stitching-in\" of pre-trained adapters for different tasks and languages.\n",
    "The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\n",
    "Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.\n",
    "Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.\n",
    "AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf8391cc-96ff-4a91-b880-2ef5753d5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all FutureWarnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ac1e2cc-569d-4c64-86aa-f68b74882796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùî What are Adapters?\n",
      "üí° small learnt bottleneck layers\n",
      "\n",
      "‚ùî What do Adapters avoid?\n",
      "üí° full fine-tuning of the entire model.\n",
      "\n",
      "‚ùî What is proposed?\n",
      "üí° AdapterHub\n",
      "\n",
      "‚ùî What does AdapterHub allow?\n",
      "üí° dynamic \"stitching-in\"\n",
      "\n",
      "‚ùî Where can I find AdapterHub?\n",
      "üí° AdapterHub.ml.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def answer_questions(questions):\n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(\"‚ùî\", question)\n",
    "        print(\"üí°\", result[\"answer\"])\n",
    "        print()\n",
    "\n",
    "answer_questions([\n",
    "    \"What are Adapters?\",\n",
    "    \"What do Adapters avoid?\",\n",
    "    \"What is proposed?\",\n",
    "    \"What does AdapterHub allow?\",\n",
    "    \"Where can I find AdapterHub?\",\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
