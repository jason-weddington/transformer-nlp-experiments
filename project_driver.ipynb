{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7fedd8-6a85-4783-b9e9-ccaf71fa30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqq adapter-transformers datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport adapter_utils\n",
    "%aimport mlm\n",
    "from adapter_utils import get_model, get_tokenizer, adapt_model, get_test_data\n",
    "from mlm import masked_language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca3eee-831e-4c87-867f-ab777ba2aac9",
   "metadata": {},
   "source": [
    "### Load and Adapt a Model\n",
    "I've written helper functions to generalize / abstract the loading of the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001bc77-84d4-427b-ab10-59fc36004197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f29687-2618-4b58-9130-2472a83b37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed052-76b4-477b-9766-696bbe8d06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_model(model=model, adapter_name=\"qa/squad1@ukp\", adapter_arch=\"houlsby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9eb656-b483-4b9e-b334-aea5892b1064",
   "metadata": {},
   "source": [
    "### Test that it's working\n",
    "Test that things are working by running the Q&A example from Adapter Hub's [sample notebook](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7550802-83ac-487d-95d6-021490d2691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline\n",
    "\n",
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "context = \"\"\"\n",
    "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.\n",
    "Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.\n",
    "Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model.\n",
    "However, sharing and integrating adapter layers is not straightforward.\n",
    "We propose AdapterHub, a framework that allows dynamic \"stitching-in\" of pre-trained adapters for different tasks and languages.\n",
    "The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\n",
    "Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.\n",
    "Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.\n",
    "AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8391cc-96ff-4a91-b880-2ef5753d5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all FutureWarnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1e2cc-569d-4c64-86aa-f68b74882796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(questions):\n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(\"â”\", question)\n",
    "        print(\"ðŸ’¡\", result[\"answer\"])\n",
    "        print()\n",
    "\n",
    "answer_questions([\n",
    "    \"What are Adapters?\",\n",
    "    \"What do Adapters avoid?\",\n",
    "    \"What is proposed?\",\n",
    "    \"What does AdapterHub allow?\",\n",
    "    \"Where can I find AdapterHub?\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b825b-0401-4e80-8cfd-58eb9e9d8cd2",
   "metadata": {},
   "source": [
    "### List the datasets that exist at HuggingFace\n",
    "HuggingFace makes it easy to access public NLP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c6a7b-deb3-4fdf-ad68-05dc184253fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "hf_data = datasets.list_datasets()\n",
    "for data in hf_data:\n",
    "    if \"glue\" in data.lower():\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9efe8-437b-42ea-830e-1d5262d6afe8",
   "metadata": {},
   "source": [
    "### Test some data\n",
    "Test things with the rotten tomatoes example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dac4c-a968-40cc-9a69-8abcfae209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4566c8-9786-4b93-86f9-7da4987a503c",
   "metadata": {},
   "source": [
    "### Test training\n",
    "try the training procedure here: https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b3fe1-6c73-4850-a908-2e56f26c73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = RobertaModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"rotten_tomatoes\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"rotten_tomatoes\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"ðŸ‘Ž\", 1: \"ðŸ‘\"}\n",
    "  )\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd444d37-caca-4358-a8e3-2fa7d7271f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e30cb-6a24-4dc6-856f-94e286abd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c87217-e769-416a-9596-815fb7d360b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e3409-1933-428e-a5ed-85a45390a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
    "\n",
    "classifier(\"This is awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb06ee-132d-4641-8146-1fb5dc582aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(\"./final_adapter\", \"rotten_tomatoes\")\n",
    "\n",
    "!ls -lh final_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fad1f-7519-4d6f-9427-103226b8aada",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### The Arvix dataset is going to take more prep work\n",
    "You have to manually download and extract this dataset, and even then I haven't been able to get it work yet. There's some nuance in the way you have to tell HuggingFace to split the data and I haven't gotten this part working yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccc049-3d55-414a-adef-6489901f5dee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, SplitInfo\n",
    "arvix_data = load_dataset(\"arxiv_dataset\", data_dir=\"./arvix_data/\", split=SplitInfo(name='train', num_bytes=2246545603, num_examples=1796911, dataset_name='arxiv_dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcec48a-b1fa-433b-b45b-d150ed17a90d",
   "metadata": {},
   "source": [
    "### Training a GLUE Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83531fe5-3808-4cc2-a16c-5a604d479a62",
   "metadata": {},
   "source": [
    "Using `run_glue_alt.py` from the Adapter Transformers repo, we can easily create an adapter for one of the GLUE tasks. Here's an example from their documentation:\n",
    "\n",
    "```\n",
    "export TASK_NAME=mrpc\n",
    "\n",
    "python run_glue_alt.py \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_train_epochs 10.0 \\\n",
    "  --output_dir /tmp/$TASK_NAME \\\n",
    "  --overwrite_output_dir \\\n",
    "  --train_adapter \\\n",
    "  --adapter_config pfeiffer\n",
    "```\n",
    "\n",
    "For convenience, I've created a shell script that launches the training process to create an adapter for the CoLA task called `cola_adapter.sh` the contents of which are:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "export TASK_NAME=cola\n",
    "\n",
    "python run_glue_alt.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --num_train_epochs 10.0 \\\n",
    "  --output_dir ./adapter/$TASK_NAME \\\n",
    "  --overwrite_output_dir \\\n",
    "  --train_adapter \\\n",
    "  --adapter_config pfeiffer\n",
    "```\n",
    "\n",
    "You can run the script by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eff50f-9897-40a1-87ef-e4730635f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh ./cola_adapter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cdaba-3efa-4aa1-bd19-16f108a77223",
   "metadata": {},
   "source": [
    "### Train an adapter for The Stanford Sentiment Treebank dataset\n",
    "1. Create `sst_adapter.sh` using `cola_adapter.sh` as an example, but train for the SST task.\n",
    "1. Push the newly created `sst_adapter.sh` file into GitLab\n",
    "1. Run the script to train the adapter. (in the cell below)\n",
    "1. Download the trained adapter from ./adapter/sst and store it in our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fffe4-f118-4a62-861f-8eb38abbbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh ./sst_adapter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947b9ba-b64e-42dc-b7f5-8fde73b264fc",
   "metadata": {},
   "source": [
    "### Exploring Unmasking with Vannila RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7fa8c76-a969-424a-9958-db5751b179e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "# unmasker = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fa81a85-a9a5-4040-b99c-e866773e5a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4859ddd2-80a3-4b67-b5fb-8c204634e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    :param sentence: sentence without punctuation\n",
    "    Randomly masks one word in a sentence\n",
    "    \"\"\"\n",
    "    words = str.split(sentence, \" \")\n",
    "    idx = randint(0, len(words) - 1)\n",
    "    orig_word = words[idx]\n",
    "    words[idx] = \"<mask>\"\n",
    "    masked = \"\"\n",
    "    for word in words:\n",
    "        masked += \" \" + word\n",
    "    return masked, idx, orig_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "680f7dac-f696-41a8-a157-43a0079eb946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:\t17 plus 4 equals <mask>.\n",
      "\n",
      "Prediction:\t17 plus 4 equals 17.\n",
      "Score:\t\t15.90%\n",
      "\n",
      "Prediction:\t17 plus 4 equals 18.\n",
      "Score:\t\t7.56%\n",
      "\n",
      "Prediction:\t17 plus 4 equals 16.\n",
      "Score:\t\t4.89%\n",
      "\n",
      "Prediction:\t17 plus 4 equals 24.\n",
      "Score:\t\t3.18%\n",
      "\n",
      "Prediction:\t17 plus 4 equals 20.\n",
      "Score:\t\t3.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = \"Don't forget to tip the waiter!\"\n",
    "# masked_sentence, index, orignal = random_mask(sample_sentence)\n",
    "masked_sentence = \"17 plus 4 equals <mask>.\"\n",
    "predictions = unmasker(masked_sentence)\n",
    "\n",
    "print(f\"Sentence:\\t{masked_sentence}\\n\")\n",
    "for prediction in predictions:\n",
    "    print(f\"Prediction:\\t{prediction.get('sequence')}\\nScore:\\t\\t{prediction.get('score') * 100:.2f}%\\n\")\n",
    "#     if prediction.get(\"token_str\") == orignal:\n",
    "#         print(\"CORRECT\")\n",
    "#     else:\n",
    "#         print(\"INCORRECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51019e26-9590-4444-9912-7ad2c01c1f2f",
   "metadata": {},
   "source": [
    "### Testing the refactored MLM script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0580386f-1ee1-4e42-ae3f-2c7ba10088ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlm import masked_language_modeling\n",
    "from mlm_utils import ModelArguments, DataTrainingArguments\n",
    "from transformers import TrainingArguments, MultiLingAdapterArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cc64174-fc9e-4757-8668-15b8f658be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:784] 2021-07-23 16:24:43,711 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:680] 2021-07-23 16:24:43,712 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "dataset = \"cola\"\n",
    "\n",
    "model = ModelArguments(\n",
    "    model_name_or_path=\"roberta-base\",\n",
    ")\n",
    "\n",
    "data = DataTrainingArguments(\n",
    "    dataset_name=\"glue\",\n",
    "    dataset_config_name=dataset,\n",
    ")\n",
    "\n",
    "training = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    overwrite_output_dir=True,\n",
    "    output_dir=f\"./adapter/mlm/{dataset}\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=10,\n",
    ")\n",
    "\n",
    "adapter = MultiLingAdapterArguments(\n",
    "    train_adapter=True,\n",
    "    adapter_config=\"pfeiffer+inv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b514a6b3-b3c2-4cf4-bd1d-cdc6179d5bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/23/2021 16:24:43 - WARNING - mlm -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "07/23/2021 16:24:43 - INFO - mlm -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./adapter/mlm/cola/runs/Jul23_16-24-43_alienware-r12,\n",
      "logging_first_step=False,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=10,\n",
      "output_dir=./adapter/mlm/cola,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=cola,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=None,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./adapter/mlm/cola,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/23/2021 16:24:44 - WARNING - datasets.builder -   Reusing dataset glue (/home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:531] 2021-07-23 16:24:44,856 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-23 16:24:44,856 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:427] 2021-07-23 16:24:44,954 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:531] 2021-07-23 16:24:45,066 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/jason/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:569] 2021-07-23 16:24:45,067 >> Model config RobertaConfig {\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"2.1.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,668 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/jason/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,669 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/jason/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,669 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/jason/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,669 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,669 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-07-23 16:24:45,670 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|modeling_utils.py:1163] 2021-07-23 16:24:45,797 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /home/jason/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1349] 2021-07-23 16:24:46,554 >> All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "[INFO|modeling_utils.py:1357] 2021-07-23 16:24:46,554 >> All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "[INFO|configuration.py:260] 2021-07-23 16:24:46,558 >> Adding adapter 'glue'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ec10cef38504e666.arrow\n",
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-88a93ce67a4e181d.arrow\n",
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-231f91ea1ab5bd4f.arrow\n",
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cf457f7d064846e8.arrow\n",
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-012200867ffd1987.arrow\n",
      "07/23/2021 16:24:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/jason/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-955c7ae95bba23dc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-23 16:24:46,712 >> The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:1199] 2021-07-23 16:24:46,717 >> ***** Running training *****\n",
      "[INFO|trainer.py:1200] 2021-07-23 16:24:46,718 >>   Num examples = 185\n",
      "[INFO|trainer.py:1201] 2021-07-23 16:24:46,718 >>   Num Epochs = 10\n",
      "[INFO|trainer.py:1202] 2021-07-23 16:24:46,718 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1203] 2021-07-23 16:24:46,718 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1204] 2021-07-23 16:24:46,718 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1205] 2021-07-23 16:24:46,719 >>   Total optimization steps = 240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 00:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1403] 2021-07-23 16:25:25,758 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1989] 2021-07-23 16:25:25,759 >> Saving model checkpoint to ./adapter/mlm/cola\n",
      "[INFO|loading.py:59] 2021-07-23 16:25:25,760 >> Configuration saved in ./adapter/mlm/cola/glue/adapter_config.json\n",
      "[INFO|loading.py:72] 2021-07-23 16:25:25,799 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_adapter.bin\n",
      "[INFO|loading.py:59] 2021-07-23 16:25:25,800 >> Configuration saved in ./adapter/mlm/cola/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-23 16:25:26,016 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_model_head.bin\n",
      "[INFO|loading.py:59] 2021-07-23 16:25:26,017 >> Configuration saved in ./adapter/mlm/cola/glue/head_config.json\n",
      "[INFO|loading.py:72] 2021-07-23 16:25:26,228 >> Module weights saved in ./adapter/mlm/cola/glue/pytorch_model_head.bin\n",
      "[INFO|tokenization_utils_base.py:1948] 2021-07-23 16:25:26,229 >> tokenizer config file saved in ./adapter/mlm/cola/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1954] 2021-07-23 16:25:26,229 >> Special tokens file saved in ./adapter/mlm/cola/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =   666311GF\n",
      "  train_loss               =       1.25\n",
      "  train_runtime            = 0:00:39.03\n",
      "  train_samples            =        185\n",
      "  train_samples_per_second =     47.388\n",
      "  train_steps_per_second   =      6.148\n",
      "07/23/2021 16:25:26 - INFO - mlm -   *** Evaluate ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:546] 2021-07-23 16:25:26,292 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:2239] 2021-07-23 16:25:26,294 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2241] 2021-07-23 16:25:26,294 >>   Num examples = 22\n",
      "[INFO|trainer.py:2244] 2021-07-23 16:25:26,294 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_loss               =     1.6797\n",
      "  eval_runtime            = 0:00:00.20\n",
      "  eval_samples            =         22\n",
      "  eval_samples_per_second =    105.802\n",
      "  eval_steps_per_second   =     14.428\n",
      "  perplexity              =      5.364\n"
     ]
    }
   ],
   "source": [
    "train_stats, eval_stats = masked_language_modeling(\n",
    "    model_args=model, data_args=data, training_args=training, adapter_args=adapter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2026ebfa-55f6-404d-802a-a65418933cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 39.0394,\n",
       " 'train_samples_per_second': 47.388,\n",
       " 'train_steps_per_second': 6.148,\n",
       " 'total_flos': 715446823680000.0,\n",
       " 'train_loss': 1.250021235148112,\n",
       " 'epoch': 10.0,\n",
       " 'train_samples': 185}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88dafe51-777c-4379-aa66-8d41a4435cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.679701566696167,\n",
       " 'eval_runtime': 0.2079,\n",
       " 'eval_samples_per_second': 105.802,\n",
       " 'eval_steps_per_second': 14.428,\n",
       " 'epoch': 10.0,\n",
       " 'eval_samples': 22,\n",
       " 'perplexity': 5.3639549494375895}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d57ea2-732b-4ea0-83d4-c63782ac9d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
