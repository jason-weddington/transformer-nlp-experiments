{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7fedd8-6a85-4783-b9e9-ccaf71fa30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -Uqq adapter-transformers datasets\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport adapter_utils\n",
    "from adapter_utils import get_model, get_tokenizer, adapt_model, get_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca3eee-831e-4c87-867f-ab777ba2aac9",
   "metadata": {},
   "source": [
    "### Load and Adapt a Model\n",
    "I've written helper functions to generalize / abstract the loading of the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001bc77-84d4-427b-ab10-59fc36004197",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f29687-2618-4b58-9130-2472a83b37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed052-76b4-477b-9766-696bbe8d06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_model(model=model, adapter_name=\"qa/squad1@ukp\", adapter_arch=\"houlsby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9eb656-b483-4b9e-b334-aea5892b1064",
   "metadata": {},
   "source": [
    "### Test that it's working\n",
    "Test that things are working by running the Q&A example from Adapter Hub's [sample notebook](https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7550802-83ac-487d-95d6-021490d2691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import QuestionAnsweringPipeline\n",
    "\n",
    "qa = QuestionAnsweringPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "context = \"\"\"\n",
    "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters.\n",
    "Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks.\n",
    "Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model.\n",
    "However, sharing and integrating adapter layers is not straightforward.\n",
    "We propose AdapterHub, a framework that allows dynamic \"stitching-in\" of pre-trained adapters for different tasks and languages.\n",
    "The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages.\n",
    "Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure.\n",
    "Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios.\n",
    "AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8391cc-96ff-4a91-b880-2ef5753d5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all FutureWarnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1e2cc-569d-4c64-86aa-f68b74882796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_questions(questions):\n",
    "    for question in questions:\n",
    "        result = qa(question=question, context=context)\n",
    "        print(\"‚ùî\", question)\n",
    "        print(\"üí°\", result[\"answer\"])\n",
    "        print()\n",
    "\n",
    "answer_questions([\n",
    "    \"What are Adapters?\",\n",
    "    \"What do Adapters avoid?\",\n",
    "    \"What is proposed?\",\n",
    "    \"What does AdapterHub allow?\",\n",
    "    \"Where can I find AdapterHub?\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993b825b-0401-4e80-8cfd-58eb9e9d8cd2",
   "metadata": {},
   "source": [
    "### List the datasets that exist at HuggingFace\n",
    "HuggingFace makes it easy to access public NLP datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3c6a7b-deb3-4fdf-ad68-05dc184253fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "hf_data = datasets.list_datasets()\n",
    "for data in hf_data:\n",
    "    if \"glue\" in data.lower():\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9efe8-437b-42ea-830e-1d5262d6afe8",
   "metadata": {},
   "source": [
    "### Test some data\n",
    "Test things with the rotten tomatoes example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dac4c-a968-40cc-9a69-8abcfae209b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4566c8-9786-4b93-86f9-7da4987a503c",
   "metadata": {},
   "source": [
    "### Test training\n",
    "try the training procedure here: https://github.com/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b3fe1-6c73-4850-a908-2e56f26c73a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = RobertaModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"rotten_tomatoes\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"rotten_tomatoes\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"üëé\", 1: \"üëç\"}\n",
    "  )\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd444d37-caca-4358-a8e3-2fa7d7271f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_accuracy(p: EvalPrediction):\n",
    "  preds = np.argmax(p.predictions, axis=1)\n",
    "  return {\"acc\": (preds == p.label_ids).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    compute_metrics=compute_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988e30cb-6a24-4dc6-856f-94e286abd47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c87217-e769-416a-9596-815fb7d360b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e3409-1933-428e-a5ed-85a45390a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
    "\n",
    "classifier(\"This is awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fb06ee-132d-4641-8146-1fb5dc582aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter(\"./final_adapter\", \"rotten_tomatoes\")\n",
    "\n",
    "!ls -lh final_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fad1f-7519-4d6f-9427-103226b8aada",
   "metadata": {},
   "source": [
    "### The Arvix dataset is going to take more prep work\n",
    "You have to manually download and extract this dataset, and even then I haven't been able to get it work yet. There's some nuance in the way you have to tell HuggingFace to split the data and I haven't gotten this part working yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfccc049-3d55-414a-adef-6489901f5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, SplitInfo\n",
    "arvix_data = load_dataset(\"arxiv_dataset\", data_dir=\"./arvix_data/\", split=SplitInfo(name='train', num_bytes=2246545603, num_examples=1796911, dataset_name='arxiv_dataset'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcec48a-b1fa-433b-b45b-d150ed17a90d",
   "metadata": {},
   "source": [
    "### Training a GLUE Adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83531fe5-3808-4cc2-a16c-5a604d479a62",
   "metadata": {},
   "source": [
    "Using `run_glue_alt.py` from the Adapter Transformers repo, we can easily create an adapter for one of the GLUE tasks. Here's an example from their documentation:\n",
    "\n",
    "```\n",
    "export TASK_NAME=mrpc\n",
    "\n",
    "python run_glue_alt.py \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_train_epochs 10.0 \\\n",
    "  --output_dir /tmp/$TASK_NAME \\\n",
    "  --overwrite_output_dir \\\n",
    "  --train_adapter \\\n",
    "  --adapter_config pfeiffer\n",
    "```\n",
    "\n",
    "For convenience, I've created a shell script that launches the training process to create an adapter for the CoLA task called `cola_adapter.sh` the contents of which are:\n",
    "\n",
    "```\n",
    "#!/bin/bash\n",
    "export TASK_NAME=cola\n",
    "\n",
    "python run_glue_alt.py \\\n",
    "  --model_name_or_path roberta-base \\\n",
    "  --task_name $TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 128 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 1e-5 \\\n",
    "  --num_train_epochs 10.0 \\\n",
    "  --output_dir ./adapter/$TASK_NAME \\\n",
    "  --overwrite_output_dir \\\n",
    "  --train_adapter \\\n",
    "  --adapter_config pfeiffer\n",
    "```\n",
    "\n",
    "You can run the script by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eff50f-9897-40a1-87ef-e4730635f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh ./cola_adapter.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cdaba-3efa-4aa1-bd19-16f108a77223",
   "metadata": {},
   "source": [
    "### Train an adapter for The Stanford Sentiment Treebank dataset\n",
    "1. Create `sst_adapter.sh` using `cola_adapter.sh` as an example, but train for the SST task.\n",
    "1. Push the newly created `sst_adapter.sh` file into GitLab\n",
    "1. Run the script to train the adapter. (in the cell below)\n",
    "1. Download the trained adapter from ./adapter/sst and store it in our Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6fffe4-f118-4a62-861f-8eb38abbbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh ./sst_adapter.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa8c76-a969-424a-9958-db5751b179e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
